<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Some notes and solutions to Russell and Norvig's Artificial Intelligence: A Modern Approach (AIMA, 3rd edition)</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Some notes and solutions to Russell and Norvig's Artificial Intelligence: A Modern Approach (AIMA, 3rd edition)"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-08-20 04:49:35 PDT"/>
<meta name="author" content=""/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Some notes and solutions to Russell and Norvig's Artificial Intelligence: A Modern Approach (AIMA, 3rd edition)</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 1.1</a></li>
<li><a href="#sec-2">2 1.2</a></li>
<li><a href="#sec-3">3 1.3</a></li>
<li><a href="#sec-4">4 1.4</a></li>
<li><a href="#sec-5">5 1.5</a></li>
<li><a href="#sec-6">6 1.6</a></li>
<li><a href="#sec-7">7 1.7</a></li>
<li><a href="#sec-8">8 1.8</a></li>
<li><a href="#sec-9">9 1.9</a></li>
<li><a href="#sec-10">10 1.10</a></li>
<li><a href="#sec-11">11 1.11</a></li>
<li><a href="#sec-12">12 1.12</a></li>
<li><a href="#sec-13">13 1.13</a></li>
<li><a href="#sec-14">14 1.14</a></li>
<li><a href="#sec-15">15 1.15</a></li>
<li><a href="#sec-16">16 2.1</a></li>
<li><a href="#sec-17">17 2.2</a>
<ul>
<li><a href="#sec-17-1">17.1 a</a></li>
<li><a href="#sec-17-2">17.2 b</a></li>
<li><a href="#sec-17-3">17.3 c</a></li>
</ul>
</li>
<li><a href="#sec-18">18 2.3</a></li>
<li><a href="#sec-19">19 2.4</a>
<ul>
<li><a href="#sec-19-1">19.1 Soccer</a></li>
<li><a href="#sec-19-2">19.2 Titan</a></li>
<li><a href="#sec-19-3">19.3 Shopping on the internet</a></li>
<li><a href="#sec-19-4">19.4 Playing a tennis match</a></li>
<li><a href="#sec-19-5">19.5 Practicing tennis against a wall</a></li>
<li><a href="#sec-19-6">19.6 Performing a high jump</a></li>
<li><a href="#sec-19-7">19.7 Knitting a sweater</a></li>
<li><a href="#sec-19-8">19.8 Bidding on an item</a></li>
</ul>
</li>
<li><a href="#sec-20">20 2.5</a></li>
<li><a href="#sec-21">21 2.6</a></li>
<li><a href="#sec-22">22 2.7</a>
<ul>
<li><a href="#sec-22-1">22.1 Goal-based agent</a></li>
<li><a href="#sec-22-2">22.2 Utility-based agent</a></li>
</ul>
</li>
<li><a href="#sec-23">23 2.8</a></li>
<li><a href="#sec-24">24 2.9</a></li>
<li><a href="#sec-25">25 2.10</a>
<ul>
<li><a href="#sec-25-1">25.1 a</a></li>
<li><a href="#sec-25-2">25.2 b</a></li>
<li><a href="#sec-25-3">25.3 c</a></li>
</ul>
</li>
<li><a href="#sec-26">26 2.11</a>
<ul>
<li><a href="#sec-26-1">26.1 a</a></li>
<li><a href="#sec-26-2">26.2 b</a></li>
<li><a href="#sec-26-3">26.3 c</a></li>
<li><a href="#sec-26-4">26.4 d</a></li>
</ul>
</li>
<li><a href="#sec-27">27 2.12</a></li>
<li><a href="#sec-28">28 2.13</a>
<ul>
<li><a href="#sec-28-1">28.1 a</a></li>
<li><a href="#sec-28-2">28.2 b</a></li>
</ul>
</li>
<li><a href="#sec-29">29 3.1</a></li>
<li><a href="#sec-30">30 3.2</a>
<ul>
<li><a href="#sec-30-1">30.1 a</a></li>
<li><a href="#sec-30-2">30.2 b</a></li>
<li><a href="#sec-30-3">30.3 c</a></li>
<li><a href="#sec-30-4">30.4 d</a></li>
</ul>
</li>
<li><a href="#sec-31">31 3.3</a>
<ul>
<li><a href="#sec-31-1">31.1 a</a></li>
<li><a href="#sec-31-2">31.2 b</a></li>
<li><a href="#sec-31-3">31.3 c</a></li>
<li><a href="#sec-31-4">31.4 d</a></li>
</ul>
</li>
<li><a href="#sec-32">32 3.7</a></li>
<li><a href="#sec-33">33 Meetups</a>
<ul>
<li><a href="#sec-33-1">33.1 Mon Jun 11 2012</a></li>
<li><a href="#sec-33-2">33.2 Mon Jun 18 2012</a>
<ul>
<li><a href="#sec-33-2-1">33.2.1 Test a simple agent in each (Python, Java, Clojure) implementation.</a></li>
<li><a href="#sec-33-2-2">33.2.2 Get some standard cables to connect to the projector.</a></li>
<li><a href="#sec-33-2-3">33.2.3 See if we can use <code>xrandr</code> to get twin-view with an external HDMI.</a></li>
</ul>
</li>
<li><a href="#sec-33-3">33.3 Mon Jun 25 2012</a>
<ul>
<li><a href="#sec-33-3-1">33.3.1 Discussion</a></li>
<li><a href="#sec-33-3-2">33.3.2 Get a minimal Clojure example up here.</a></li>
<li><a href="#sec-33-3-3">33.3.3 Set up <code>csrg.org</code> with a mailing list.</a></li>
</ul>
</li>
<li><a href="#sec-33-4">33.4 Mon Jul  2 2012</a></li>
<li><a href="#sec-33-5">33.5 Tue Jul 24 2012</a></li>
<li><a href="#sec-33-6">33.6 Mon Aug  6 2012</a>
<ul>
<li><a href="#sec-33-6-1">33.6.1 3.1</a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-34">34 Notes</a>
<ul>
<li><a href="#sec-34-1">34.1 1</a></li>
<li><a href="#sec-34-2">34.2 2</a></li>
<li><a href="#sec-34-3">34.3 3</a></li>
<li><a href="#sec-34-4">34.4 Lectures</a>
<ul>
<li><a href="#sec-34-4-1">34.4.1 1</a></li>
<li><a href="#sec-34-4-2">34.4.2 2</a></li>
</ul>
</li>
<li><a href="#sec-34-5">34.5 Turing, Computing Machinery and Intelligence</a></li>
</ul>
</li>
<li><a href="#sec-35">35 TODOs</a>
<ul>
<li><a href="#sec-35-1">35.1 Focus on one or two problems for the coming week.</a></li>
<li><a href="#sec-35-2">35.2 Modify 2.11 to list available actions?</a></li>
<li><a href="#sec-35-3">35.3 A listing environment</a></li>
<li><a href="#sec-35-4">35.4 Blog about barabási-albert vs. depth-first graph-creation.</a></li>
<li><a href="#sec-35-5">35.5 2.11</a>
<ul>
<li><a href="#sec-35-5-1">35.5.1 Graph world</a></li>
<li><a href="#sec-35-5-2">35.5.2 Depth-first graph constructor</a></li>
<li><a href="#sec-35-5-3">35.5.3 Allow specification of random-seeds</a></li>
<li><a href="#sec-35-5-4">35.5.4 Stateful graph agent</a></li>
<li><a href="#sec-35-5-5">35.5.5 <code>make-equilibrium-limited-environment</code></a></li>
<li><a href="#sec-35-5-6">35.5.6 Compare stateful and randomized reflex agents.</a></li>
<li><a href="#sec-35-5-7">35.5.7 Figure out the correct aspect ratio for youtube.</a></li>
</ul>
</li>
<li><a href="#sec-35-6">35.6 Should we structure this somehow as a blog instead of an org-doc?</a></li>
<li><a href="#sec-35-7">35.7 Some sort of blog post or other publicity?</a></li>
<li><a href="#sec-35-8">35.8 Find a reasonable pseudocode package in \LaTeX.</a></li>
<li><a href="#sec-35-9">35.9 Should we tangle to a bunch of text files?</a></li>
<li><a href="#sec-35-10">35.10 Reimplement the Lisp environment in Scheme.</a></li>
<li><a href="#sec-35-11">35.11 Personal notes as footnotes.</a></li>
<li><a href="#sec-35-12">35.12 Should we try to release an e.g. Wumpus World egg?</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> <span class="done DONE">DONE</span> 1.1</h2>
<div class="outline-text-2" id="text-1">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2011-10-10 Mon 03:03</span></span><br/>
</p><dl>
<dt>Intelligence</dt><dd>A spontaneous faculty for associating impressions
                    (more general than ideas); synthesizing
                    abstractions from disparate stimuli; deducing
                    conclusions from abstractions.

<p>
                    Intelligence is an emergent property of simples
                    like e.g. neurons.
</p></dd>
<dt>Artificial intelligence</dt><dd>Mechanism for performing association,
       abstraction, deduction which appears to be spontaneous; may
       also be an emergent property of bit-pushing.
</dd>
<dt>Agent</dt><dd>Self-contained, autonomous input-processing mechanism.
</dd>
<dt>Rationality</dt><dd>The appropriate application of <i>λόγος</i> or <i>ratio</i>;
                   this includes the mechanical process of deduction,
                   as well as an ill-defined notion of common-sense.
</dd>
<dt>Logical reasoning</dt><dd>The mechanical aspect of rationality.
</dd>
</dl>

</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> <span class="done DONE">DONE</span> 1.2</h2>
<div class="outline-text-2" id="text-2">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2011-10-10 Mon 03:03</span></span><br/>
  The Mathematical Objection (3) still holds up: the halting problem;
  on the other hands, humans are also susceptible to the halting
  problem, aren't they? If one falls towards the humanity side of the
  humanity-rationality AI-axis, this deficit is reducible.
</p>
<p>
  Lady Lovelace's Objection (6) is interesting: it denies <i>ex nihilo</i>;
  are genetic algorithms a counter-example?
</p>
<p>
  The Argument from Informality of Behaviour (8) could be solved by
  fuzzy dispatch.
</p>
<p>
  A modern refutation might be that there are not enough graduate
  students to make a satisfactory ontology of world-knowledge; thank
  the gods, then, for mechanical turks and unsupervised learning!
</p>
<p>
  We came pretty damn close to \(30\%\) in the 2008 <a href="http://en.wikipedia.org/wiki/Loebner_Prize#2008">Loebner prize</a>; why not
  double it to \(60\%\) in 2058? Despite Moore's law, let's say that AI
  proceeds linearly.
</p></div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> <span class="done DONE">DONE</span> 1.3</h2>
<div class="outline-text-2" id="text-3">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2011-10-12 Wed 12:58</span></span><br/>
  Reflex actions are rational in the sense that they are the result of
  induction on e.g. hot objects and the scientific method (see
  Turing); though the acquisition may require intelligence (induction,
  storage), the reflex itself is not intelligent in the sense that it
  requires no induction: it is immediate.
</p>
<p>
  Reflex actions are not irrational, either, in the sense that someone
  does a cost-benefit analysis and decides to contravene it; let's
  call reflex actions <i>pararational</i>, therefore: neither rational nor
  irrational. There's no time to apply a utility function and behave
  accordingly (or discordingly (sic)).
</p></div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> <span class="done DONE">DONE</span> 1.4</h2>
<div class="outline-text-2" id="text-4">

<p>  Tom Evan's ANALOGY is an ad-hoc geometric solver, and would not
  therefore program. In people, you might be able to generalize from
  IQ-tests to success; but not so with domain-specific AI.
</p></div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> <span class="done DONE">DONE</span> 1.5</h2>
<div class="outline-text-2" id="text-5">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-28 Mon 21:35</span></span><br/>
  Aplysia, Eric Kandel
</p>
<p>
  20,000 neurons; memory-updates/second: \(10^{-9}\); cycle time:
  \(10^15\), high end;
</p>
<p>
  Is memory-updates/second merely memory / cycle time? In which case:
  \(20000 / 10^-9 = 10^5 (20000)\) neurons, cycle time: \(10^{-3}\);
  memory updates per second? Not sure what the relationship between
  operations/sec and memory updates/sec; the former is an upper bound,
  though. Could it be that memory updates/sec is also bounded,
  somehow, by storage units? There is also the relationship between
  neuros and synapses.
</p>
<p>
  In humans, <a href="http://en.wikipedia.org/wiki/Neurons#Connectivity">7,000 synapses per neuron</a>; hence 10<sup>14</sup> from 10<sup>11</sup>. How
  many synapses per aplysia-neuron?
</p>
<p>
  From <a href="http://learnmem.cshlp.org/content/10/5/387.full">this paper</a>:
</p>
<blockquote>

<p>On average, we found 24 contacts per pair of neurons.
</p>
</blockquote>


<p>
  Let's say, then, that sea slugs have 10<sup>6</sup> synapses; let's also say
  that, like humans, this is an upper bound on memory updates per
  second due to the e.g. <a href="http://en.wikipedia.org/wiki/Action_potential#Refractory_period">refractory period</a>.
</p>
<p>
  That gives 10<sup>6</sup> memory updates per second; which means that a
  supercomputer houses the potential of 10<sup>8</sup> sea slugs.
</p></div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> <span class="done DONE">DONE</span> 1.6</h2>
<div class="outline-text-2" id="text-6">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-28 Mon 21:43</span></span><br/>
  This post on the <a href="http://lesswrong.com/lw/6p6/the_limits_of_introspection/">limits of introspection</a> posits that:
</p>
<blockquote>

<p>Mental processes are the results of opaque preferences, and . . .
our own "introspected" goals and preferences are a product of the
same machinery that infers goals and preferences in others in order
to predict their behavior.
</p>
</blockquote>


<p>
  Accordingly, introspection is accurate to the extent that we can
  infer our own thoughts from the mental model we've extrapolated from
  watching others.
</p>
<p>
  In other words, the processes which lead to thought are to thought
  opaque.
</p></div>

</div>

<div id="outline-container-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> <span class="done DONE">DONE</span> 1.7</h2>
<div class="outline-text-2" id="text-7">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:17</span></span><br/>
  Bar code scanners should hopefully be a trivial mapping from codes
  to products; if, on the other hand, you could scan and select
  similar products someone might be interested in: well, then.
</p>
<p>
  The search engine problem is probably AI-complete; current solutions
  are some AI-complete-like heuristics.
</p>
<p>
  Voice-activated telephone menus might be artificially intelligent in
  the sense that they have to recover signal from noise and make sense
  of it.
</p>
<p>
  Internet routing algorithms are classic agents in the sense that
  they have environments (connection data), sensors (the ability to
  peer into network devices) and actuators (the ability to re-route
  traffic).
</p></div>

</div>

<div id="outline-container-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> <span class="done DONE">DONE</span> 1.8</h2>
<div class="outline-text-2" id="text-8">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:17</span></span><br/>
  Isn't it the case that humans do do some kind of implicit
  calculation? Another example is the ability to catch a ball: there
  are complex physics at play, and yet the human has evolutionarily
  honed and ad-hoc facilities to perform the same.
</p>
<p>
  Something like Gaussian blur, in other words, is hard-coded into our
  neurons; vision system, on the other hand, don't have the advantage
  of fuzzy connections between analog neurons and have to simulate
  these biological heuristics with algorithms.
</p></div>

</div>

<div id="outline-container-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> <span class="done DONE">DONE</span> 1.9</h2>
<div class="outline-text-2" id="text-9">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:19</span></span><br/>
  Evolution might tend to result in systems that maximize certain
  utility functions (e.g. propagate genes, to that end: stay alive for
  a while; &amp;c.); this process is pseudo-rational. Pseudo-rational in
  the sense that it is not rational for rationality's sake; but
  accidentally rational as it strives to maximize utility.
</p>
<p>
  Maybe there's no distinction to be drawn there after all: ends
  justifying means.
</p></div>

</div>

<div id="outline-container-10" class="outline-2">
<h2 id="sec-10"><span class="section-number-2">10</span> <span class="done DONE">DONE</span> 1.10</h2>
<div class="outline-text-2" id="text-10">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:28</span></span><br/>
  AI is science in the sense that it benefits from the scientific
  method (work done, for instance, on the relationship between goals
  and actions; cooperation; how brains cause minds; &amp;c.) and precise
  mathematics.
</p>
<p>
  AI is engineering, on the other hand, in the sense that it inheres
  in the world; it must find solutions in messy situations: solutions
  which might be approximate but nevertheless useful.
</p></div>

</div>

<div id="outline-container-11" class="outline-2">
<h2 id="sec-11"><span class="section-number-2">11</span> <span class="done DONE">DONE</span> 1.11</h2>
<div class="outline-text-2" id="text-11">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:35</span></span><br/>
  "Surely computers . . . can do only what their programmers tell
  them" might have been the case, if it weren't for the fact that
  programmers can program machines to do things even they couldn't do
  (cf. chess programs that outstrip their masters).<sup><a class="footref" name="fnr.1" href="#fn.1">1</a></sup>
</p>
<p>
  This seems like a paradox I don't adequately know how to explain; if
  it proceeds apace, prepare for the <a href="http://en.wikipedia.org/wiki/Technological_singularity">singularity</a>.
</p></div>

</div>

<div id="outline-container-12" class="outline-2">
<h2 id="sec-12"><span class="section-number-2">12</span> <span class="done DONE">DONE</span> 1.12</h2>
<div class="outline-text-2" id="text-12">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:41</span></span><br/>
  The relationship between nature and nurture is probably complex;
  suffice to say: genes might provide an upper bound on the
  intelligence of an animal that it has to strive to meet. Luck helps;
  so does discipline.
</p>
<p>
  There is a nature-nuture/code-intelligence analogy only insofar as
  there is code that adapts to its environment; or a programmer can
  translate intelligence into code (bounded by the programmer's
  intelligence, of course).
</p></div>

</div>

<div id="outline-container-13" class="outline-2">
<h2 id="sec-13"><span class="section-number-2">13</span> <span class="done DONE">DONE</span> 1.13</h2>
<div class="outline-text-2" id="text-13">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 02:48</span></span><br/>
  It's true that animals, humans and computers are bound by the laws
  of physics; nevertheless, there is this bizarre phenomenon of
  <a href="http://en.wikipedia.org/wiki/Emergence#Emergence_in_humanity">emergent behavior</a> wherein the sum is more than its whole of parts.
</p>
<p>
  Consciousness, after all, is an emergent behavior from the
  propagation of current through neurons; and the world-wide-web has
  emerged from a decentralized connection of web pages.
</p></div>

</div>

<div id="outline-container-14" class="outline-2">
<h2 id="sec-14"><span class="section-number-2">14</span> <span class="done DONE">DONE</span> 1.14</h2>
<div class="outline-text-2" id="text-14">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2011-10-10 Mon 03:52</span></span><br/>
</p><ol>
<li>The <a href="http://www.youtube.com/watch?v=NZZOgT8oct4">Japanese</a> got this one; just a toy, though.
</li>
<li>There is at least one <a href="http://www.egitmagazine.com/2011/07/06/doip-drive-over-ip-a-new-egyptian-technology/">driverless car</a> in Cairo; it's not
     self-controlling, though, but rather remotely driven. Driving in
     clusterfuck-Cairo (like Athens) is taxing for humans, let alone
     AI. (Google's making <a href="http://news.cnet.com/8301-17852_3-20074383-71/google-good-news-nevadas-yes-to-driverless-cars/">political inroads</a> in Nevada, though.)
     Sufficiently sensitive sensation of surrounding objects,
     conditions; physics; navigation; are required.
</li>
<li><a href="http://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)">DARPA Grand Challenge</a>
</li>
<li>This robot <a href="http://singularityhub.com/2011/10/08/robot-i-now-have-common-sense-engineer-great-go-fetch-me-a-sandwich/">fetches a sandwich</a>.
</li>
<li><a href="http://lifehacker.com/5610363/grocery-iq-is-a-brilliant-grocery-list-application">Grocery IQ</a> will order groceries; a week's worth, though?
</li>
<li><a href="http://en.wikipedia.org/wiki/Computer_bridge#Computers_versus_humans">Zia Mahmood</a> got clowned once or twice; like poker, though, bridge
     is probabilistic and psychological.
</li>
<li><a href="http://theorymine.co.uk/">TheoryMine</a> is selling new computer-generated proofs for £15;
     <a href="http://en.wikipedia.org/wiki/Computer-assisted_proof#Philosophical_objections">standard objections</a> apply.
</li>
<li>The Bulhak-Larios <a href="http://www.elsewhere.org/pomo/">Postmodernism Generator</a> is funny; intentionally
     so?
</li>
<li>Hilariously-named <a href="http://en.wikipedia.org/wiki/Shyster_(expert_system)">SHYSTER</a>: ad-hoc expert system
</li>
<li><a href="https://market.android.com/details?id=com.google.android.apps.translate&amp;hl=en">Google Translate</a>
</li>
<li>Mechanically, but there is a human agent (telemanipulator); see
      <a href="http://en.wikipedia.org/wiki/Robotic_surgery#Timeline">this</a>, though, where "In May 2006 the first AI doctor-conducted
      unassisted robotic surgery on a 34 year old male to correct
      heart arythmia."
</li>
</ol>

</div>

</div>

<div id="outline-container-15" class="outline-2">
<h2 id="sec-15"><span class="section-number-2">15</span> <span class="done DONE">DONE</span> 1.15</h2>
<div class="outline-text-2" id="text-15">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-31 Thu 03:05</span></span><br/>
  <a href="http://en.wikipedia.org/wiki/Text_Retrieval_Conference">TREC</a> appears to dissolve tracks as they become "solved" (e.g. the
  spam and terabyte tracks) and take new ones up as they emerge (e.g.
  the microblog and crowdsourcing tracks).
</p>
<p>
  The <a href="http://en.wikipedia.org/wiki/DARPA_Grand_Challenge">Grand Challenge</a> is attempting to solve the problem of driverless
  transportation (see Google's <a href="http://en.wikipedia.org/wiki/Google_driverless_car">driverless car</a>); despite recent
  <a href="http://www.driverlesscarhq.com/driverless-car-legislation-sweeps-california-senate-37-0/">legislation</a> approving driverless cars (in e.g. California, Nevada,
  New Jersey), it is still cutting edge.
</p>
<p>
  <a href="http://icaps12.poli.usp.br/icaps12/ickeps">ICKEPS 2012</a>, for instance, has a track for planning solar array
  operations on the ISS; seems relevant.
</p>
<p>
  <a href="http://en.wikipedia.org/wiki/RoboCup">RoboCup</a> is interesting in the sense that it requires advanced
  perception and cooperation among autonomous agents; I suspect it
  does not detract much from new ideas, despite the fact that it is
  still wrestling with some of the oldest (and unsolved) problems in
  AI (<i>vide supra</i>).
</p>
<p>
  The <a href="http://www.loebner.net/Prizef/loebner-prize.html">Loebner Prize</a>, on the other hand, seems a little anachronistic;
  do people care whether their AI counterparts really act human?
</p></div>

</div>

<div id="outline-container-16" class="outline-2">
<h2 id="sec-16"><span class="section-number-2">16</span> <span class="done DONE">DONE</span> 2.1</h2>
<div class="outline-text-2" id="text-16">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-11 Mon 00:51</span></span><br/>
  It follows directly from the definition of a rational agent, which
  "maximizes its performance measure, given the evidence provided by
  the percept sequence" (p. 37), that its action "depends . . . on the
  time step it has reached."
</p>
<p>
  This is because the lifetime of an agent is measured by the total
  number of percepts it receives <sup><a class="footref" name="fnr.2" href="#fn.2">2</a></sup> <sup><a class="footref" name="fnr.3" href="#fn.3">3</a></sup>.
</p>
<p>
  Let \(t\) be the time step the agent has reached; if \(t \leq T\), the
  agent's performence measure depends upon the time step it has
  reached. If \(t &gt; T\), on the other hand, the rationality of the agent
  is undefined; since its performance measure is undefined.
</p>
<p>
  At \(t &gt; T\), the agent has become pararational (neither rational nor
  irrational).
</p>
<p>
  A rational agent's action, therefore, depends upon \(t\) only insofar
  as its performance measure depends upon \(t\).
</p>
<p>
  Take <a href="http://en.wikipedia.org/wiki/Opportunity_rover">Opportunity</a>, for instance, which had a performance measure of
  \(T = 90\ \text{sol}\); as of 2012, it's overstepped \(T\) by eight
  years. If it fails after \(T\) to e.g. characterize soil, could you
  say that it acts rationally? In other words, is <a href="http://en.wikipedia.org/wiki/Spirit_rover">Spirit</a> irrational;
  now that it has failed to meet its original performance measure?
</p>
<p>
  No: by their original performance measure, Opportunity and Spirit
  are pararational; which is not to say that you couldn't define
  another performance measure \(u^\prime\) depending upon another time
  \(T^\prime\).
</p>
<p>
  See page 38, by the way, where the authors talk about rationality in
  terms of expected performance; could it be that an agent transcends
  \(T\) with respect to expected performance?
</p>
<p>
  Example: given a penalty for each move, a reflex agent's expected
  performance would be just as good as any other's given T = 2; but
  not when T = 1000 (it would require a state-based agent to realize
  that the world is clean and stop moving).
</p></div>

</div>

<div id="outline-container-17" class="outline-2">
<h2 id="sec-17"><span class="section-number-2">17</span> <span class="done DONE">DONE</span> 2.2</h2>
<div class="outline-text-2" id="text-17">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-11 Mon 17:18</span></span><br/>
</p>
</div>

<div id="outline-container-17-1" class="outline-3">
<h3 id="sec-17-1"><span class="section-number-3">17.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-17-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-11 Mon 17:18</span></span><br/>
   Page 38 describes an environment which is partially observable,
   deterministic and static; as such, the tabular agent in Fig. 2.3
   can expect to maximize its utility in no more than four actions
   (the worst case is A: dirty, B: dirty; which results in either
   <code>suck, right, suck, left, ...</code> or <code>suck, left, suck, right, ...</code>).
</p>
<p>
   There is no time to e.g. build a model of dirt, since the dirt
   doesn't replentish itself.
</p></div>

</div>

<div id="outline-container-17-2" class="outline-3">
<h3 id="sec-17-2"><span class="section-number-3">17.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-17-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-11 Mon 17:18</span></span><br/>
   The agent does require internal state: it should know, for
   instance, whether it has cleaned every square; and, if so, should
   stop.
</p></div>

</div>

<div id="outline-container-17-3" class="outline-3">
<h3 id="sec-17-3"><span class="section-number-3">17.3</span> <span class="done DONE">DONE</span> c</h3>
<div class="outline-text-3" id="text-17-3">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-11 Mon 17:18</span></span><br/>
   It should learn the geography of its environment to avoid wasting
   time trying to move off of it; it could maintain, furthermore, a
   dirt-distribution across the grid and favor those squares that tend
   to get dirty.
</p></div>
</div>

</div>

<div id="outline-container-18" class="outline-2">
<h2 id="sec-18"><span class="section-number-2">18</span> <span class="done DONE">DONE</span> 2.3</h2>
<div class="outline-text-2" id="text-18">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-13 Wed 05:40</span></span><br/>
<a name="2.3a" id="2.3a"></a>
</p><dl>
<dt>a</dt><dd>False. Page 42 mentions that, even in unobservable
         environments, "the agent's goals may still be achievable,
         sometimes with certainty;" the reflexive vacuum agent on page
         38 is an example.
</dd>
<dt>b</dt><dd>True. In an unknown environment, there is no opportunity for
         the reflex agent to learn the ``laws of physics'' of the
         environment (p. 44); or for the programmer to endow the agent
         with them <i>a priori</i>.
</dd>
<dt>c</dt><dd>True. It's possible to imagine a task environment in which
         there are no decisions to be made: merely existing, for
         instance, satisfies the performance measure.
</dd>
<dt>d</dt><dd>False. According to page 46, the agent program takes the
         current percept; the agent function, on the other hand, takes
         the entire percept history.
</dd>
<dt>e</dt><dd>False. If the agent function is to e.g. determine whether a
         program will return an answer or run forever (see p. 8); it
         is not implementable by a program/machine combination.
         Unless, of course, the author (or agent) has solved the
         <a href="http://en.wikipedia.org/wiki/Halting_problem">halting problem</a>.
</dd>
<dt>f</dt><dd>True. Take the performance measure, for instance, where an
         agent is supposed to simulate the roll of a <a href="http://en.wikipedia.org/wiki/Halting_problem">fair-sided die</a>.
</dd>
<dt>g</dt><dd>True. If an agent is a rational NxN tic-tac-toe player, it
         will perform just as well in a 2x2 as in a 3x3 environment.
</dd>
<dt>h</dt><dd>False. See <a href="#2.3a">a</a>: page 138 describes a sensorless vacuum agent
         that knows the geography of its world; it's possible to
         search its belief space and even coerce the world into
         certain states.
</dd>
<dt>i</dt><dd>False. Even rational poker-playing agents fall prey to luck.
</dd>
</dl>

</div>

</div>

<div id="outline-container-19" class="outline-2">
<h2 id="sec-19"><span class="section-number-2">19</span> <span class="done DONE">DONE</span> 2.4</h2>
<div class="outline-text-2" id="text-19">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-13 Wed 06:47</span></span><br/>
</p>
</div>

<div id="outline-container-19-1" class="outline-3">
<h3 id="sec-19-1"><span class="section-number-3">19.1</span> Soccer</h3>
<div class="outline-text-3" id="text-19-1">

<dl>
<dt>Performance measure</dt><dd>Score and defend
</dd>
<dt>Environment</dt><dd>Field
</dd>
<dt>Actuators</dt><dd>Kicking, thrwing, catching
</dd>
<dt>Sensors</dt><dd>Topology, ball, agents
</dd>
<dt>Characteristics</dt><dd>Fully observable, multiagent, stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-2" class="outline-3">
<h3 id="sec-19-2"><span class="section-number-3">19.2</span> Titan</h3>
<div class="outline-text-3" id="text-19-2">

<dl>
<dt>Performance measure</dt><dd>Like <a href="http://en.wikipedia.org/wiki/Titan_Mare_Explorer">TiME</a> for surface lakes, it would
       determine the presence of biological compounds.
</dd>
<dt>Environment</dt><dd>Titan
</dd>
<dt>Actuators</dt><dd>Drill, satellite, landing gear
</dd>
<dt>Sensors</dt><dd>Mass spectrometer, camera
</dd>
<dt>Characteristics</dt><dd>Partially observable, multiagent? stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-3" class="outline-3">
<h3 id="sec-19-3"><span class="section-number-3">19.3</span> Shopping on the internet</h3>
<div class="outline-text-3" id="text-19-3">

<dl>
<dt>Performance measure</dt><dd>Finding used AI books
</dd>
<dt>Environment</dt><dd>The internet
</dd>
<dt>Actuators</dt><dd>Form completion, HTTP request, cookie storage
</dd>
<dt>Sensors</dt><dd>HTML parser
</dd>
<dt>Characteristics</dt><dd>Partially observable, multiagent, stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-4" class="outline-3">
<h3 id="sec-19-4"><a name="2.4-tennis" id="2.4-tennis"></a><span class="section-number-3">19.4</span> Playing a tennis match</h3>
<div class="outline-text-3" id="text-19-4">

<dl>
<dt>Performance measure</dt><dd>Winning the match
</dd>
<dt>Environment</dt><dd>Tennis court
</dd>
<dt>Actuators</dt><dd>Tennis racket
</dd>
<dt>Sensors</dt><dd>Location, trajectory of ball, opponent; topology
</dd>
<dt>Characteristics</dt><dd>Fully observable, multiagent, stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-5" class="outline-3">
<h3 id="sec-19-5"><span class="section-number-3">19.5</span> Practicing tennis against a wall</h3>
<div class="outline-text-3" id="text-19-5">

<dl>
<dt>Performance measure</dt><dd>Length of rally
</dd>
<dt>Environment</dt><dd>Half-court with wall
</dd>
<dt>Actuators</dt><dd>See <a href="#sec-19-4">above</a>.
</dd>
<dt>Sensors</dt><dd>See <a href="#sec-19-4">above</a> (sans opponent).
</dd>
<dt>Characteristics</dt><dd>Fully observable, single agent, stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-6" class="outline-3">
<h3 id="sec-19-6"><span class="section-number-3">19.6</span> Performing a high jump</h3>
<div class="outline-text-3" id="text-19-6">

<dl>
<dt>Performance measure</dt><dd>Height jumped
</dd>
<dt>Environment</dt><dd>Measuring stick
</dd>
<dt>Actuators</dt><dd>Spring
</dd>
<dt>Sensors</dt><dd>Balance
</dd>
<dt>Characteristics</dt><dd>Fully observable, single agent, deterministic,
       episodic, static, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-7" class="outline-3">
<h3 id="sec-19-7"><span class="section-number-3">19.7</span> Knitting a sweater</h3>
<div class="outline-text-3" id="text-19-7">

<dl>
<dt>Performance measure</dt><dd>Consistency of stitch, conformance to the
       recipient's body
</dd>
<dt>Environment</dt><dd>Yarn, recipient's body
</dd>
<dt>Actuators</dt><dd>Needle
</dd>
<dt>Sensors</dt><dd>Yarn on needle
</dd>
<dt>Characteristics</dt><dd>Fully observable, single agent, deterministic,
       sequential, static, continuous, known
</dd>
</dl>

</div>

</div>

<div id="outline-container-19-8" class="outline-3">
<h3 id="sec-19-8"><span class="section-number-3">19.8</span> Bidding on an item</h3>
<div class="outline-text-3" id="text-19-8">

<dl>
<dt>Performance measure</dt><dd>Win, save cash
</dd>
<dt>Environment</dt><dd>Auction
</dd>
<dt>Actuators</dt><dd>Signify bid
</dd>
<dt>Sensors</dt><dd>See the artifact, understand the auctioneer
</dd>
<dt>Characteristics</dt><dd>Partially observable <sup><a class="footref" name="fnr.4" href="#fn.4">4</a></sup>, stochastic,
       sequential, dynamic, continuous, known
</dd>
</dl>

</div>
</div>

</div>

<div id="outline-container-20" class="outline-2">
<h2 id="sec-20"><span class="section-number-2">20</span> <span class="done DONE">DONE</span> 2.5</h2>
<div class="outline-text-2" id="text-20">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-14 Thu 06:44</span></span><br/>
</p><dl>
<dt>Agent</dt><dd>An agent is a black box with inputs and outputs that
             conspires to perform something
</dd>
<dt>Agent function</dt><dd>The agent function maps inputs to outputs.
</dd>
<dt>Agent program</dt><dd>The agent program implements the agent function.
</dd>
<dt>Rationality</dt><dd>Rationality usually means the application of
                   reason; but because the authors have given up on AI
                   as "thinking humanly" (p. 2), it has been cheapened
                   to mean: "act in accordance with this performance
                   measure we've set up."
</dd>
<dt>Autonomy</dt><dd>Autonomy is the ability of an agent to select actions
                beyond the <i>a priori</i> programming of its maker.
</dd>
<dt>Reflex agent</dt><dd>A reflex agent acts according to the immediate
                    percept; it has amnesia.
</dd>
<dt>Model-based agent</dt><dd>A model-based agent acts according to a model
       of the world it has synthesized from percepts.
</dd>
<dt>Goal-based agent</dt><dd>Not merely reacting to the environment (or its
       model thereof), the goal-based agent has a Vorhaben (so to
       speak) that can inform sequences of actions.
</dd>
<dt>Utility-based agent</dt><dd>Utility-based agents have internalized
       their own performance measure; and, as such, are able to decide
       between conflicting goals.
</dd>
<dt>Learning agent</dt><dd>Learning agents hone their sense of appropriate
                      actions by modifying the weights associated with
                      environmental features.
</dd>
</dl>

</div>

</div>

<div id="outline-container-21" class="outline-2">
<h2 id="sec-21"><span class="section-number-2">21</span> <span class="done DONE">DONE</span> 2.6</h2>
<div class="outline-text-2" id="text-21">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-14 Thu 12:06</span></span><br/>
</p><dl>
<dt>a</dt><dd>There are infinite agent programs that implement a given
         agent function; take, for instance, an agent that perceives
         flashes of light and maps them to some output (say, an
         integer).

<p>         
         The percept sequence could be mapped as an integer encoded as
         a bit-string of light and dark moments; or a bit-array
         representing the same thing.
</p></dd>
<dt>b</dt><dd>Yes; an agent function whose performance measure is to
         determine whether a program stops or not cannot be
         implemented as a progrm (unless one first solves the Halting
         Problem).
</dd>
<dt>c</dt><dd>Yes; which is to say: a program implements a mapping from
         percepts to actions; to change the mapping, you have to
         change the program.
</dd>
<dt>d</dt><dd>There would be \(2^n\) possible agent programs on an \(n\)-bit
         machine (not all of them functional).

<p>         
         (According to <a href="http://www.ics.uci.edu/~welling/teaching/ICS171fall10/cs171_hw1_solutions.pdf">this</a>, there are \(a^{2^n}\) possible programs;
         \(2^n\) possible states and \(a\) choices for each state. I don't
         think they're factoring the program into the storage, are
         they?)
</p></dd>
<dt>e</dt><dd>Speeding up the agent program does not change the agent
         function; they are orthogonal: the former is concrete, the
         latter abstract.

<p>         
         If they don't behave like Hegelian dialectic, they are at
         least Platonic forms and instantiations.
</p></dd>
</dl>

</div>

</div>

<div id="outline-container-22" class="outline-2">
<h2 id="sec-22"><span class="section-number-2">22</span> <span class="done DONE">DONE</span> 2.7</h2>
<div class="outline-text-2" id="text-22">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-20 Wed 03:52</span></span><br/>
  Clever: the goal-based agent mutates belief-space based on its best
  guess; acts accordingly.
</p>
</div>

<div id="outline-container-22-1" class="outline-3">
<h3 id="sec-22-1"><span class="section-number-3">22.1</span> Goal-based agent</h3>
<div class="outline-text-3" id="text-22-1">




<pre class="example">- let
  - state
  - model
  - goals
  - action
    - define (goal-based-agent percept)
      - set! state (update-state state action percept model)
      - let
        # Shouldn't we distinguish between many different
        action-sequences; and, if so, how to do so without a utility
        function: evaluate them against the performance measure?
        - action-sequence (search goals state)
          - return (first action-sequence)
</pre>

</div>

</div>

<div id="outline-container-22-2" class="outline-3">
<h3 id="sec-22-2"><span class="section-number-3">22.2</span> Utility-based agent</h3>
<div class="outline-text-3" id="text-22-2">




<pre class="example">- let
  - state
  - model
  - goals
  - action
    - define (utility-based-agent percept)
      - set! state (update-state state action percept model)
      - let
        - probabilities (map probability goals)
        - utilities (map utility goals)
          - let
            - expected-utilities (map * probabilities utilities)
            - goal-of-maximum-expected-utility (max goals expected-utilities)
            - action-sequence (search goal-of-maximum-expected-utility state)
              - return (first action-sequence)
</pre>

</div>
</div>

</div>

<div id="outline-container-23" class="outline-2">
<h2 id="sec-23"><span class="section-number-2">23</span> <span class="done DONE">DONE</span> 2.8</h2>
<div class="outline-text-2" id="text-23">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-21 Thu 04:39</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-06-21 Thu 04:39</span></span> <br/>
    See <a href="https://github.com/klutometis/aima-chicken">aima-chicken</a>.
</li>
</ul>




<pre class="example">(use debug
     foof-loop
     lolevel
     srfi-1
     srfi-8
     srfi-13
     srfi-69
     vector-lib)

(define (simulate environment)
  (loop ((while (environment)))))

(define (compose-environments . environments)
  (lambda ()
    (every identity (map (lambda (environment)
                           (environment))
                         environments))))

(define (make-performance-measuring-environment
         measure-performance
         score-update!)
  (lambda () (score-update! (measure-performance))))

(define (make-step-limited-environment steps)
  (let ((current-step 0))
    (lambda ()
      (set! current-step (+ current-step 1))
      (&lt; current-step steps))))

;;; What about pairs of objects and optional display things.
(define make-debug-environment
  (case-lambda
   ((object) (make-debug-environment object pp))
   ((object display)
    (lambda () (display object)))))

(define (vacuum-world-display world)
  (pp
   (vector-append '#(world)
                  (vector-map
                   (lambda (i clean?)
                     (if clean? 'clean 'dirty))
                   world))))

(define clean #t)
(define clean? identity)

(define dirty #f)
(define dirty? (complement clean?))

(define left 0)
(define left? zero?)

(define right 1)
(define right? (complement zero?))

(define make-vacuum-world vector)

(define vacuum-world-location vector-ref)

(define vacuum-world-location-set! vector-set!)

(define-record vacuum-agent
  location
  score
  program)

(define-record-printer vacuum-agent
  (lambda (vacuum-agent output)
    (format output
            "#(agent ~a ~a)"
            (if (left? (vacuum-agent-location vacuum-agent))
                'left
                'right)
            (vacuum-agent-score vacuum-agent))))

(define (make-vacuum-environment world agent)
  (lambda ()
    (let* ((location (vacuum-agent-location agent))
           (action ((vacuum-agent-program agent)
                    location
                    (vacuum-world-location world location))))
      (case action
        ((left) (vacuum-agent-location-set! agent left))
        ((right) (vacuum-agent-location-set! agent right))
        ((suck) (vacuum-world-location-set! world location clean))
        (else (error (string-join
                      "make-vacuum-environment --"
                      "Unknown action")
                     action))))))

(define (reflex-vacuum-agent-program location clean?)
  (if clean?
      (if (left? location)
          'right
          'left)
      'suck))

(define make-reflex-vacuum-agent
  (case-lambda
   ((location)
    (make-reflex-vacuum-agent location reflex-vacuum-agent-program))
   ((location program)
    (make-vacuum-agent
     location
     0
     program))))

(define (make-vacuum-performance-measure world)
  (lambda ()
    (vector-count (lambda (i square) (clean? square)) world)))

(define (make-vacuum-score-update! agent)
  (lambda (score)
    (vacuum-agent-score-set! agent (+ (vacuum-agent-score agent)
                                      score))))

(define simulate-vacuum
  (case-lambda
   ((world agent) (simulate-vacuum world agent 1000))
   ((world agent steps)
    (simulate
     (compose-environments
      (make-step-limited-environment steps)
      (make-performance-measuring-environment
       (make-vacuum-performance-measure world)
       (make-vacuum-score-update! agent))
      (make-debug-environment agent)
      (make-debug-environment world vacuum-world-display)
      (make-vacuum-environment world agent)))
    (vacuum-agent-score agent))))

(simulate-vacuum (make-vacuum-world dirty clean)
                 (make-reflex-vacuum-agent
                  left
                  (lambda (location clean?)
                    'right))
                 10)
</pre>


<p>
  I want environmental combinators, incidentally; such that I can
  compose an e.g. step-limited environment with an agent with a vacuum
  one.
</p>
<p>
  We can compose steps; but how do you compose score: do you have to
  specify a reducer of some kind; e.g. addition? Is it really
  environment reduction we're talking about here?
</p>
<p>
  I'm beginning to suspect that the performance score is a property of
  the agent, not the environment; this is consistent with the book's
  use of "reward" and "penalty." It also makes sense in a multi-agent
  environment.
</p>
<p>
  On page 37, however, the authors state that:
</p>
<blockquote>

<p>This notion of desirability [for a sequence of actions leading to a
sequence of states] is captured by a <b>performance measure</b> that
evaluates any given sequence of environment states.
</p>
</blockquote>


<p>
  I suspect that, whereas the environment is an arbiter of the
  performance score (i.e. applies the performance measure), the score
  inheres in the agents.
</p>
<p>
  This is corroborated by the following:
</p>
<blockquote>

<p>Notice that we said <i>environment</i> states, not <i>agent</i> states. If we
define success in terms of agent's opinion of its own performance,
an agent could achieve perfect rationality simply by deluding itself
that its performance was perfect.
</p>
</blockquote>


<p>
  Since only the environment has access to its true states, it alone
  can measure performance. Is this problematic in cases where we don't
  have an omniscient environment that directly communicates
  performance scores? In such cases, we'd have to rely on the
  imperfect self-judgement of the agent; and attempt to converge on
  rationality by internal coherence.
</p>
<p>
  What I'm calling environments, incidentally, are now just functions:
  step-functions, at that; and can be reduced by <code>every</code>.
</p>
<p>
  Agent combinators are a little tough, though; the performance
  measure has to be aware of the combined features. Can we use some
  kind of message-passing mechanism?
</p>
<p>
  What stops us, for instance, as modelling the agents as lambdas;
  too? Part of the problem is the inversion of control: we'd have to
  pass a message to the agent to store its score, as opposed to
  manipulating the score directly.
</p>
<p>
  Every agent would be a dispatch-mechanism that would manage its own
  meta-variables (including score and e.g. location) on the basis of
  messages. Is it problematic, however, to have agents managing their
  own score? Could we have an agent &rarr; score mapping in the environment
  itself? That way, agents only maintain state according to its
  percepts.
</p>
<p>
  Score, for instance, is not a percept in the vacuum world; location,
  however, is. Agents, then, are functions with closures; functions
  which take as many parameters as their percepts have components. The
  performance-measuring-environment, therefore, maintains an
  <code>agent-&gt;score</code> table. Yes!
</p>
<p>
  Problem is, though, that we'd have to break the nice contract we
  have: environments are niladic lambdas. To maintain the performance
  measure table, we'd have to receive the agent and the new score.
</p>
<p>
  How to make the performance measure part of the environment, so that
  we can relieve the agent from metadata?
</p>
<p>
  By taking the metadata out of the agent, we have to maintain agent
  &rarr; metadata mappings in the environment; this is kind of a pain in
  the ass.
</p>
<p>
  By maintaining agents-as-lambda, we get a certain flexibility; on
  the other hand, we shunt some complexity onto the environment: as it
  has to maintain agent-metadata: score, location, &amp;c.
</p>
<p>
  Is this an acceptable tradeoff? The alternative, where I need to
  guess what agents need (program, score, location) seems onerous; for
  some reason. In practice, however, it may be simpler. We haven't
  even solved the agent-hashing-problem, for instance (wherein hashing
  fails if we mutate a field).
</p>
<p>
  Can we hash closures?
</p>
<p>
  I want to follow this environment-maintains-agent-&gt;metadata-mapping
  thing and see how far it goes. (I see now why objects are
  interesting; closures, of course, do the same thing.)
</p>
<p>
  If <code>make-*-environment</code> returned multiple values: the thunk followed
  by e.g. <code>agent-&gt;score</code>, <code>agent-&gt;location</code>; you can ignore the latter
  values, if you want to.
</p>
<p>
  Or, we can demand that the user furnish them; better yet, we can
  give the user the option of furnishing and ignoring them.
</p>
<p>
  Also, shouldn't we be able to name agents at some point? This would
  also have to fall within an external data structure. Maybe the
  record solution isn't problematic if we create ad-hoc agents for
  each problem.
</p>
<p>
  If we really need to decouple the program from the agent metadata
  (do we?), one solution is to have an agent-&gt;metadata table in the
  environment; the metadata would be a record containing location,
  score, name, &amp;c.
</p>
<p>
  This metadata table, on the other hand, would have to be passed to
  each subenvironment for composition. Seems like a pain.
</p>
<p>
  We found that, since environments consist of a step function, we
  could reduce them to a lambda; let's see if this continues to be the
  case. For the time being, however, I think using agent-records is
  simplifying.
</p>
<p>
  I wouldn't mind agents being lambdas with closures; problem is:
  can't access the closure without some kind of message passing.
  (Message passing simulates records.) We could possibly do it with
  some kind of multiple-return-values hack, in which the subsequent
  values are ignored (the agent effectively does a state dump every
  time its program is invoked). The problem with that is that I have
  to pass a percept in to access its state, or store its state some
  other way.
</p>
<p>
  To avoid namespacing everything (like e.g. <code>vacuum-agent</code>, &amp;c.), I'd
  like to have separate modules; that way, if we need to, we can
  import with a prefix.
</p>
<p>
  For learning purposes, we should allow the student to specify no
  more than the agent program; worry about all the bootstrapping on
  the back end.
</p>
<p>
  We may have to copy worlds, incidentally, to compare how e.g.
  reflex- vs. state-agents behave; thank goodness for <code>vector-copy</code>.
  (Copy by default?)
</p>
<p>
  To give feedback to students, should have an e.g.
  <code>environment-print</code> that we can pass around (this sort of
  function-passing, incidentally, is what Norvig sought to avoid);
  <code>environment-print</code> might happen at every step in e.g. <code>simulate</code>.
  Oh, <code>make-debugging-environment</code>.
</p></div>

</div>

<div id="outline-container-24" class="outline-2">
<h2 id="sec-24"><span class="section-number-2">24</span> <span class="done DONE">DONE</span> 2.9</h2>
<div class="outline-text-2" id="text-24">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-28 Thu 12:47</span></span><br/>
  Using the <a href="https://github.com/klutometis/aima-chicken">aima-chicken</a> framework:
</p>



<pre class="example">(use aima-vacuum
     test)

(let ((worlds
       (list (make-world clean clean)
             (make-world clean clean)
             (make-world clean dirty)
             (make-world clean dirty)
             (make-world dirty clean)
             (make-world dirty clean)
             (make-world dirty dirty)
             (make-world dirty dirty)))
      (agents
       (list (make-reflex-agent left)
             (make-reflex-agent right)
             (make-reflex-agent left)
             (make-reflex-agent right)
             (make-reflex-agent left)
             (make-reflex-agent right)
             (make-reflex-agent left)
             (make-reflex-agent right))))
  (let* ((scores (map simulate-vacuum worlds agents))
         (average-score (/ (apply + scores) 8)))
    (test
     "Scores for each configuration"
     scores
     '(2000 2000 1998 1999 1999 1998 1996 1996))
    (test
     "Average overall score"
     1998.25
     average-score)))
</pre>

</div>

</div>

<div id="outline-container-25" class="outline-2">
<h2 id="sec-25"><span class="section-number-2">25</span> <span class="done DONE">DONE</span> 2.10</h2>
<div class="outline-text-2" id="text-25">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-29 Fri 17:52</span></span><br/>
</p>
</div>

<div id="outline-container-25-1" class="outline-3">
<h3 id="sec-25-1"><span class="section-number-3">25.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-25-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-29 Fri 13:29</span></span><br/>
   With a partially observable environment, a simple reflex agent will
   not be rational (in the sense that its expected performance is not
   as good as any other's); in other words, it should be scoring about
   twice as much as this:
</p>



<pre class="example">(use aima-vacuum
     test)

(test
 "Penalizing vacuum with reflex agent"
 998
 (simulate-penalizing-vacuum (make-world dirty dirty)
                             (make-reflex-agent left)))     
</pre>


<p>
   The reflex agent would require state to determine that e.g. the
   world was clean and that it didn't need to move anymore.
</p></div>

</div>

<div id="outline-container-25-2" class="outline-3">
<h3 id="sec-25-2"><span class="section-number-3">25.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-25-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-29 Fri 17:52</span></span><br/>
</p>


<pre class="example">(use aima
     aima-vacuum
     test
     vector-lib)

(debug? #f)

(define-record unknown)

(define unknown (make-unknown))

(define (all-clean? world)
  ;; Vector bleeds a little world.
  (vector-every (lambda (location) (clean? location)) world))

(test
 "Stateful agent in penalizing environment"
 1995
 (simulate-penalizing-vacuum
  (make-world dirty dirty)
  (make-reflex-agent
   left
   ;; We could also make an initial pessimistic hypothesis of all-dirty.
   (let ((world (make-world unknown unknown)))
     (lambda (location clean?)
       (if clean?
           (begin
             ;; Extra work here every time; otherwise, we'd have an
             ;; extra `all-clean?' check after we set the state.
             ;; `vector-set!', I'd wager, is cheaper than
             ;; `all-clean?'.
             (vector-set! world location clean)
             (if (all-clean? world)
                 ;; Symbols appropriate here, or should we have predefined
                 ;; go-left, go-right, clean, do-nothing? We're message
                 ;; passing, after all; I suppose a lambda wouldn't make any
                 ;; sense?
                 ;;
                 ;; Can't be lambdas unless we redefine e.g. `go-right'
                 ;; to penalize in the case of
                 ;; `make-penalizing-environment'; better to keep as
                 ;; symbols and dispatch, right? There should be some
                 ;; sort of data-directed model we could use, though,
                 ;; instead of the case-based dispatch.
                 'noop
                 (if (right? location)
                     'left
                     'right)))
           'suck))))))

(test
 "Stateful agent in penalizing environment (from the egg)"
 1995
 (simulate-penalizing-vacuum
  (make-world dirty dirty)
  (make-stateful-reflex-agent left)))

</pre>

</div>

</div>

<div id="outline-container-25-3" class="outline-3">
<h3 id="sec-25-3"><span class="section-number-3">25.3</span> <span class="done DONE">DONE</span> c</h3>
<div class="outline-text-3" id="text-25-3">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-29 Fri 17:52</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-06-29 Fri 17:52</span></span> <br/>
     Should we actually implement it?
</li>
</ul>

<p>   If the simple and stateful reflex agents are omniscient w.r.t. the
   environment, they are equivalent; the stateful agent will simply
   update its state according to its omniscient percept and the simple
   one will simply act accordingly.
</p></div>
</div>

</div>

<div id="outline-container-26" class="outline-2">
<h2 id="sec-26"><span class="section-number-2">26</span> <span class="done DONE">DONE</span> 2.11</h2>
<div class="outline-text-2" id="text-26">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 16:59</span></span><br/>
</p>
</div>

<div id="outline-container-26-1" class="outline-3">
<h3 id="sec-26-1"><span class="section-number-3">26.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-26-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 14:00</span></span><br/>
    A simple reflex agent wouldn't be able to explore an environment of
    unknown extent without exhausting all possible paths of the
    corresponding \(n \times n\) space; given sufficient time, such an
    exhaustive agent would asymptotically approach rationality toward \(t
    = \infty\).
</p>
<p>
    Given reasonable time constraints, however, or e.g. penalties for
    moving, such an agent would not be rational; if it maintained all
    possible paths in a table, it would also contravene the directive on
    p. 47:
</p>
<blockquote>

<p>The key challenge for AI is to find out how to write programs that,
to the extent possible, produce rational behavior from a smallish
program rather than from a vast table.
</p>
</blockquote>


<p>
    More fundamentally, an agent wouldn't be able to exhaust the space
    without maintaining some sort of state (e.g. paths traversed).
</p>
<p>
    Even more fundamentally, however, the agent can't discern whether
    it's hit a wall; this changes in <a href="#sec-27">2.12</a>, however, when the agent
    gets a bump sensor.
</p></div>

</div>

<div id="outline-container-26-2" class="outline-3">
<h3 id="sec-26-2"><a name="2.11b" id="2.11b"></a><span class="section-number-3">26.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-26-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 14:00</span></span><br/>
</p>
<p>
   The average score for a randomized agent in a 20-node world is
   roughly \(17300.0\); see the <a href="http://youtu.be/EvZvyxAoNdo">demonstration-video</a>.
</p>



<pre class="example">(use aima aima-vacuum test)

(parameterize ((current-test-epsilon 0.005))
  (test
   "Test the randomized graph agent on 100 different worlds."
   17300.0
   (let* ((scores
           (list-tabulate
            100
            (lambda (i)
              (let* ((world (make-graph-world))
                     (start (random-start world))
                     (agent (make-randomized-graph-agent start)))
                (parameterize ((random-seed i)
                               (debug? #f))
                  (simulate-graph world agent))
                (agent-score agent))))))
     (/ (apply + scores) (length scores)))))
</pre>

</div>

</div>

<div id="outline-container-26-3" class="outline-3">
<h3 id="sec-26-3"><a name="2.11c" id="2.11c"></a><span class="section-number-3">26.3</span> <span class="done DONE">DONE</span> c</h3>
<div class="outline-text-3" id="text-26-3">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 14:49</span></span><br/>
   The randomized agent will perform poorly in a linear environment,
   since many of its movement choices will be no-op; in fact, the
   average score of a randomized agent on a linear world of 20 nodes
   is roughly \(15000.0\) (\(\approx13\%\) less than the random 20-node
   world in <a href="#sec-26-2">2.11b</a>):
</p>



<pre class="example">(use aima aima-vacuum debug test)

(parameterize ((current-test-epsilon 0.1))
  (test
   "Test the randomized graph agent on a linear world 100 times."
   15000.0
   (let* ((world (make-linear-world))
          (start (random-start world)))
     (let ((scores (list-tabulate
                    100
                    (lambda (i)
                      (let* ((world (copy-world world))
                             (agent (make-randomized-graph-agent start)))
                        (parameterize ((debug? #f))
                          (simulate-graph world agent))
                        (agent-score agent))))))
       (/ (apply + scores) (length scores))))))
</pre>


<p>
   See <a href="http://youtu.be/wNLzEiIAG">the video</a>.
</p></div>

</div>

<div id="outline-container-26-4" class="outline-3">
<h3 id="sec-26-4"><span class="section-number-3">26.4</span> <span class="done DONE">DONE</span> d</h3>
<div class="outline-text-3" id="text-26-4">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 16:58</span></span><br/>
   An agent with state can outperform a stateless agent and maximize
   its performance by systematically exploring the environment á la
   e.g. depth-first search; in a 20 node environment, the stateful
   agent performs \(\approx 20\%\) better than its randomized
   counterpart (cf. <a href="#sec-26-3">2.11c</a>).
</p>
<p>
   See <a href="http://youtu.be/B28ay_zSnoY">the video</a>.
</p>



<pre class="example">(use aima aima-vacuum test)

(parameterize ((current-test-epsilon 0.005))
  (test
   "Test the stateful graph agent on 100 different worlds."
   19176.35
   (let* ((scores
           (list-tabulate
            100
            (lambda (i)
              (let* ((world (make-graph-world))
                     (start (random-start world))
                     (agent (make-stateful-graph-agent start)))
                (parameterize ((random-seed i)
                               (debug? #f))
                  (simulate-graph world agent))
                (agent-score agent))))))
     (/ (apply + scores) (length scores)))))
</pre>


<p>
   The basic algorithm is as follows:
</p>
<ol>
<li>Is the current location dirty? Clean it.
</li>
<li>Otherwise, visit and clean (if necessary) all the
      current-location's unvisited neighbors.
</li>
<li>If there are no unvisited neighbors for the current location, go
      back the way we came.
</li>
<li>If there are no unvisited (and uncleaned) locations, stop.
</li>
</ol>


<p>      
   Traversing the world in this fashion is <a href="http://en.wikipedia.org/wiki/Depth-first_search#Properties">linearly complex</a>.
</p></div>
</div>

</div>

<div id="outline-container-27" class="outline-2">
<h2 id="sec-27"><span class="section-number-2">27</span> <span class="done DONE">DONE</span> 2.12</h2>
<div class="outline-text-2" id="text-27">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 17:09</span></span><br/>
  A simple reflex agent with a bump sensor will perform just as well
  as a random agent since, upon detecting a bump, it can randomly
  change directions. The same constraints on random agent apply: e.g.
  poor performance in linear spaces.
</p>
<p>
  The state agent is fundamentally unchanged: instead of deducing
  implicit bumps, however, due to non-movement (requiring e.g. a
  bump-sentinel in the movement stack); it can incorporate the
  bump-data directly into its state.
</p>
<p>
  If the bump sensor stops working, unfortunately, the agent would
  have to fall back on random behavior (see <a href="#sec-26-2">2.11b</a>).
</p></div>

</div>

<div id="outline-container-28" class="outline-2">
<h2 id="sec-28"><span class="section-number-2">28</span> <span class="done DONE">DONE</span> 2.13</h2>
<div class="outline-text-2" id="text-28">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 17:20</span></span><br/>
</p>
</div>

<div id="outline-container-28-1" class="outline-3">
<h3 id="sec-28-1"><span class="section-number-3">28.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-28-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 17:20</span></span><br/>
   By expending an extra step at each square to make sure that it is,
   in fact, clean (and repeatedly cleaning otherwise); one can still
   clean the entire environment in linear time.
</p>
<p>
   If the dirt sensor is wrong some percentage of the time, one would
   repeatedly sense the status of the location until some significance
   criterion is reached. For instance, it appears as though one would
   have to sense the status \(17\) times per location to achieve a
   confidence level of \(95\%\).
</p></div>

</div>

<div id="outline-container-28-2" class="outline-3">
<h3 id="sec-28-2"><span class="section-number-3">28.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-28-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 17:20</span></span><br/>
   I don't see how you could avoid repeatedly exploring and cleaning
   the world according to some acceptable interval; if the dirt is not
   evenly distributed, store statistics about dirty hotspots (and hit
   those locations more frequently).
</p></div>
</div>

</div>

<div id="outline-container-29" class="outline-2">
<h2 id="sec-29"><span class="section-number-2">29</span> <span class="done DONE">DONE</span> 3.1</h2>
<div class="outline-text-2" id="text-29">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 03:42</span></span><br/>
  Formally, a well-defined problem (see page 66) contains a goal-test;
  such that we can't define a problem until we've formulated a goal.
</p>
<p>
  Page 65 states, furthermore, that ``problem formulation is the
  process of deciding what actions and states to consider, given a
  goal;'' searching for solutions can't proceed without
  goal-formulation, either.
</p></div>

</div>

<div id="outline-container-30" class="outline-2">
<h2 id="sec-30"><span class="section-number-2">30</span> <span class="done DONE">DONE</span> 3.2</h2>
<div class="outline-text-2" id="text-30">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 19:23</span></span><br/>
</p>
</div>

<div id="outline-container-30-1" class="outline-3">
<h3 id="sec-30-1"><span class="section-number-3">30.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-30-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 19:23</span></span><br/>
<a name="3.2a" id="3.2a"></a>
</p><dl>
<dt>States</dt><dd>A graph whose nodes point east, west, north, south
               (pointing to a special sentinel-node in the case of
               adjoining walls); the orientation of the robot; the
               square currently occupied by the robot.
</dd>
<dt>Initial state</dt><dd>Graph of one central node; northward orientation
</dd>
<dt>Actions</dt><dd>Turn east, west, north, south; move forward.
</dd>
<dt>Transition model</dt><dd>Moves in the currently oriented direction
        wherever a wall does not intervene; where a wall intervenes,
        however, the robot stays put.
</dd>
<dt>Goal test</dt><dd>Is the robot out of the maze?
</dd>
<dt>Path cost</dt><dd>None
</dd>
</dl>


<p>
   The state space is infinite: you can reach any square, for
   instance, over an unlimited number of noöps (e.g. pressing forward
   into a wall, turning redundantly, &amp;c.).<sup><a class="footref" name="fnr.5" href="#fn.5">5</a></sup>
</p>
<p>
   Let's say that redundant noöps have been pruned, however; and that
   the graph, furthermore, doesn't have any cycles: the state space is
   at least \(n\), corresponding to the number of nodes in the graph. It
   is at least \(4n\), though, since the robot can be in any of the four
   orientations in each square. Let's say, furthermore, for every node
   \(n_i\) there are \(m_i\) unique paths (i.e. sequences \(n_{0, 1, \dots,
   i}\)) from the initial node \(n_0\) to \(n_i\): the state space for
   \(n_i\) alone becomes all the ways to fulfill each path in \(m_i\)
   including unnecessary turns and false-forwards into walls. Let the
   set of all fulfillments for a given \(m_i\) be \(M_i\).
</p>
<p>
   The state space is the sum of \(M_i\) over the number of nodes in the
   graph; plus the robots current position; plus the robot's
   orientation.
</p></div>

</div>

<div id="outline-container-30-2" class="outline-3">
<h3 id="sec-30-2"><span class="section-number-3">30.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-30-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 19:23</span></span><br/>
<a name="3.2b" id="3.2b"></a>
</p><dl>
<dt>States</dt><dd>A graph whose nodes point to one or more of east, west,
               north, south if a neighbor exists in that direction;
               the orientation of the robot; the square currently
               occupied by the robot.
</dd>
<dt>Initial state</dt><dd>Graph of one central node; northward orientation
</dd>
<dt>Actions</dt><dd>Turn east, west, north, south if the robot is at an
                intersection; or move forward.
</dd>
<dt>Transition model</dt><dd>Moves in the currently oriented direction
        wherever a wall does not intervene; where a wall intervenes,
        however, the robot stays put.
</dd>
<dt>Goal test</dt><dd>Is the robot out of the maze?
</dd>
<dt>Path cost</dt><dd>None
</dd>
</dl>


<p>   
   The state-space is still infinite, since the problem admits of
   redundant forwards at intersections; the pruned state space is
   smaller than <a href="#3.2a">3.2a</a>, however, since the fulfillment of \(m_i\) doesn't
   involve unnecessary turns in corridors.
</p></div>

</div>

<div id="outline-container-30-3" class="outline-3">
<h3 id="sec-30-3"><span class="section-number-3">30.3</span> <span class="done DONE">DONE</span> c</h3>
<div class="outline-text-3" id="text-30-3">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 19:23</span></span><br/>
</p><dl>
<dt>States</dt><dd>A graph whose nodes point to one or more of east, west,
               north, south if a neighbor exists in that direction;
               the square currently occupied by the robot.
</dd>
<dt>Initial state</dt><dd>Graph of one central node
</dd>
<dt>Actions</dt><dd>Turn east, west, north, south
</dd>
<dt>Transition model</dt><dd>Move east, west, north, south until the robot
        is at a turning point.
</dd>
<dt>Goal test</dt><dd>Is the robot out of the maze?
</dd>
<dt>Path cost</dt><dd>None
</dd>
</dl>


<p>
   If the maze has loops, the state space is infinite; otherwise, the
   state-space is significantly smaller than even <a href="#3.2b">3.2b</a>, since \(M_i\)
   doesn't require any superfluous movement to exhaust it.
</p>
<p>
   The robot's orientation is irrelevant, since we'll travel along the
   chosen direction to the next turning point.
</p></div>

</div>

<div id="outline-container-30-4" class="outline-3">
<h3 id="sec-30-4"><span class="section-number-3">30.4</span> <span class="done DONE">DONE</span> d</h3>
<div class="outline-text-3" id="text-30-4">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-19 Sun 19:23</span></span><br/>
   At least three simplifications; that:
</p>
<ol>
<li>the maze is oriented along strict cardinal directions;
</li>
<li>the passages are straight;
</li>
<li>the passages are passable;
</li>
<li>the robot has unlimited energy.
</li>
</ol>

</div>
</div>

</div>

<div id="outline-container-31" class="outline-2">
<h2 id="sec-31"><span class="section-number-2">31</span> <span class="done DONE">DONE</span> 3.3</h2>
<div class="outline-text-2" id="text-31">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-20 Mon 04:40</span></span><br/>
</p>
</div>

<div id="outline-container-31-1" class="outline-3">
<h3 id="sec-31-1"><span class="section-number-3">31.1</span> <span class="done DONE">DONE</span> a</h3>
<div class="outline-text-3" id="text-31-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-20 Mon 04:29</span></span><br/>
</p><dl>
<dt>State</dt><dd>A state specifies the location of each friend on the map
              \(n(\{i, j\})\), and the respective paths that led them
              there \(P(\{i, j\})\).
</dd>
<dt>Initial state</dt><dd>A friend a each city \(n(\{i, j\})_0\), \(P(\{i,
                      j\})\) are empty.
</dd>
<dt>Actions</dt><dd>Each friend moves to a neighboring city <sup><a class="footref" name="fnr.6" href="#fn.6">6</a></sup>.
</dd>
<dt>Transition model</dt><dd>Each friend moves to a neighboring city.
</dd>
<dt>Goal test</dt><dd>\(n(i) = n(j)\), the two friends are in the same city.
</dd>
<dt>Path cost</dt><dd>\(max(T(i_n, i_{n+1}), T(j_n, j_{n+1}))\), where \(T\)
                  is the time required to move from one city to the
                  next.
</dd>
</dl>


<p>
   This is reminiscent of bidirectional search (\S 3.4.6) with a
   heuristic.
</p></div>

</div>

<div id="outline-container-31-2" class="outline-3">
<h3 id="sec-31-2"><span class="section-number-3">31.2</span> <span class="done DONE">DONE</span> b</h3>
<div class="outline-text-3" id="text-31-2">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-20 Mon 04:29</span></span><br/>
   An admissible heuristic is one that never overestimates the cost to
   reach a goal.
</p><ol>
<li>\(D(i, j)\), the straight-line-distance heuristic, is admissible
      (by the triangle inequality).
</li>
<li>\(2D(i, j)\), on the other hand, is not admissible; since it might
      overestimate the cost of reaching the goal.
</li>
<li>\(D(i, j)/2\) is admissible, since \(D(i, j)/2 &lt; D(i, j)\) and \(D(i,
      j)\) is admissible.
</li>
</ol>

</div>

</div>

<div id="outline-container-31-3" class="outline-3">
<h3 id="sec-31-3"><span class="section-number-3">31.3</span> <span class="done DONE">DONE</span> c</h3>
<div class="outline-text-3" id="text-31-3">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-20 Mon 04:29</span></span><br/>
   If noöp is allowed, no completely connected map exists for which
   there is no solution: one friend stays still, the other meets her;
   if noöps are not allowed, on the other hand, even a completely
   connected graph with two-nodes has no solution (since the problem
   states that \(i\) and \(j\) are different cities).
</p>


<p>
      <a href="3-3c.pdf">3-3c.pdf</a>
</p>
</div>

</div>

<div id="outline-container-31-4" class="outline-3">
<h3 id="sec-31-4"><span class="section-number-3">31.4</span> <span class="done DONE">DONE</span> d</h3>
<div class="outline-text-3" id="text-31-4">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-08-20 Mon 04:40</span></span><br/>
   With friends located at \(a\) and \(f\):
</p>

<p>
      <a href="3-3d.pdf">3-3d.pdf</a>
</p>
</div>
</div>

</div>

<div id="outline-container-32" class="outline-2">
<h2 id="sec-32"><span class="section-number-2">32</span> <span class="todo TODO">TODO</span> 3.7</h2>
<div class="outline-text-2" id="text-32">

<p>  See <a href="http://en.wikipedia.org/wiki/Convex_hull_algorithms">convex hulls</a>.
</p></div>

</div>

<div id="outline-container-33" class="outline-2">
<h2 id="sec-33"><span class="section-number-2">33</span> Meetups</h2>
<div class="outline-text-2" id="text-33">


</div>

<div id="outline-container-33-1" class="outline-3">
<h3 id="sec-33-1"><span class="section-number-3">33.1</span> Mon Jun 11 2012</h3>
<div class="outline-text-3" id="text-33-1">

<ul>
<li>Had to redefine the rational from "exercizing reason" to
     "maximizing utility function" because they gave up an AI as
     thinking machines in the 60s.
</li>
<li>Mitochondria were once autonomous agents; cells as composite
     agents
</li>
<li>Thin vs. thick agents and skynet
</li>
<li>In games like poker, the mind of the adversarial agents are part
     of the environment; requires a theory of mind to discern things
     like: "is he bluffing?"
</li>
</ul>

</div>

</div>

<div id="outline-container-33-2" class="outline-3">
<h3 id="sec-33-2"><span class="section-number-3">33.2</span> Mon Jun 18 2012</h3>
<div class="outline-text-3" id="text-33-2">

<ul>
<li>David has the international version, which would have you write
     an essay on evolution and autonomy; see e.g. Turing on child AI:

<blockquote>

<p>     We have thus divided our problem into two parts. The and
     child-programme the education process. These two remain very
     closely connected. We cannot expect to find a good child-machine
     at the first attempt. One must experiment with teaching one such
     machine and see how well it learns. One can then try another and
     see if it is better or worse. There is an obvious connection
     between this process and evolution, by the identifications
</p>
<dl>
<dt>Structure of the child machine</dt><dd>Hereditary material
</dd>
<dt>Changes</dt><dd>Mutations
</dd>
<dt>Natural selection</dt><dd>Judgment of the experimenter
</dd>
</dl>


<p>
     One may hope, however, that this process will be more expeditious
     than evolution. The survival of the fittest is a slow method for
     measuring advantages. The experimenter, by the exercise of
     intelligence, should be able to speed it up. Equally important is
     the fact that he is not restricted to random mutations. If he can
     trace a cause for some weakness he can probably think of the kind
     of mutation which will improve it.
</p>
</blockquote>


</li>
</ul>


</div>

<div id="outline-container-33-2-1" class="outline-4">
<h4 id="sec-33-2-1"><span class="section-number-4">33.2.1</span> <span class="done CANCELED">CANCELED</span> Test a simple agent in each (Python, Java, Clojure) implementation.</h4>
<div class="outline-text-4" id="text-33-2-1">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-20 Wed 03:19</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-06-20 Wed 03:19</span></span> <br/>
      Looks like we're going to standardize on Clojure.
</li>
</ul>

</div>

</div>

<div id="outline-container-33-2-2" class="outline-4">
<h4 id="sec-33-2-2"><span class="section-number-4">33.2.2</span> <span class="done DONE">DONE</span> Get some standard cables to connect to the projector.</h4>
<div class="outline-text-4" id="text-33-2-2">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-20 Wed 03:19</span></span><br/>
</p></div>

</div>

<div id="outline-container-33-2-3" class="outline-4">
<h4 id="sec-33-2-3"><span class="section-number-4">33.2.3</span> <span class="done DONE">DONE</span> See if we can use <code>xrandr</code> to get twin-view with an external HDMI.</h4>
<div class="outline-text-4" id="text-33-2-3">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-15 Fri 06:04</span></span><br/>
</p>


<pre class="example">xrandr --output eDP1 --off
xrandr --output HDMI1 --mode 1280x720
</pre>

</div>
</div>

</div>

<div id="outline-container-33-3" class="outline-3">
<h3 id="sec-33-3"><span class="section-number-3">33.3</span> Mon Jun 25 2012</h3>
<div class="outline-text-3" id="text-33-3">


</div>

<div id="outline-container-33-3-1" class="outline-4">
<h4 id="sec-33-3-1"><span class="section-number-4">33.3.1</span> Discussion</h4>
<div class="outline-text-4" id="text-33-3-1">

<ul>
<li id="sec-33-3-1-1">2.1<br/>
     Change performance measure; utility-based agent aware of its own
     performance measure: can react accordingly? Not a reflex agent,
     though, that's pre-programmed.

<p>
     Example: given a penalty for each move, a reflex agent's expected
     performance would be just as good as any other's given T = 2; but
     not when T = 1000 (it would require a state-based agent to
     realize that the world is clean and stop moving).
</p></li>
</ul>
<ul>
<li id="sec-33-3-1-2">2.2<br/>
<dl>
<dt>c</dt><dd>Memory, motor, bump sensor (or penalty); learn geography,
            probability of becoming dirty. Clustering algorithm:
            centers of mass for dirt.
</dd>
</dl>

</li>
</ul>
<ul>
<li id="sec-33-3-1-3">2.3<br/>
<dl>
<dt>a</dt><dd>Best action given available information.
</dd>
<dt>c</dt><dd>One-square, not dirty.
</dd>
<dt>d</dt><dd>Page 51 (program) vs. page 36 (function).
</dd>
<dt>f</dt><dd>See <b>c</b> above.
</dd>
<dt>g</dt><dd>Stochastic vs. deterministic vacuum world: reflex agents
            are still rational. Performance measure still the same.
</dd>
<dt>i</dt><dd>Even an omniscient poker player is subject to luck.
</dd>
</dl>

</li>
</ul>
</div>

</div>

<div id="outline-container-33-3-2" class="outline-4">
<h4 id="sec-33-3-2"><span class="section-number-4">33.3.2</span> <span class="todo TODO">TODO</span> Get a minimal Clojure example up <a href="https://github.com/klutometis/aima-clojure">here</a>.</h4>
<div class="outline-text-4" id="text-33-3-2">

</div>

</div>

<div id="outline-container-33-3-3" class="outline-4">
<h4 id="sec-33-3-3"><span class="section-number-4">33.3.3</span> <span class="todo TODO">TODO</span> Set up <code>csrg.org</code> with a mailing list.</h4>
<div class="outline-text-4" id="text-33-3-3">

<p>    This is an alternative to e.g. Google groups and whatever
    mechanism Meetup has.
</p>
<p>
    (Or is it <code>csrg.com</code>? It is indeed <code>csrg.org</code>.)
</p></div>
</div>

</div>

<div id="outline-container-33-4" class="outline-3">
<h3 id="sec-33-4"><span class="section-number-3">33.4</span> Mon Jul  2 2012</h3>
<div class="outline-text-3" id="text-33-4">

<ul>
<li>Each parameter in the agent-program corresponds to a sensor.
</li>
<li>Should we pass a status-variable into the sensor instead of a
     boolean? Might be more consistent with the singletons. Nah, fuck
     it.
</li>
</ul>

</div>

</div>

<div id="outline-container-33-5" class="outline-3">
<h3 id="sec-33-5"><span class="section-number-3">33.5</span> Tue Jul 24 2012</h3>
<div class="outline-text-3" id="text-33-5">

<ul>
<li>Michael mentioned that the triangle-inequality based consistency
     (monotonicity) condition states merely that the heuristic
     function monotonically decrease as you approach the goal.
</li>
</ul>

</div>

</div>

<div id="outline-container-33-6" class="outline-3">
<h3 id="sec-33-6"><span class="section-number-3">33.6</span> Mon Aug  6 2012</h3>
<div class="outline-text-3" id="text-33-6">


</div>

<div id="outline-container-33-6-1" class="outline-4">
<h4 id="sec-33-6-1"><span class="section-number-4">33.6.1</span> 3.1</h4>
<div class="outline-text-4" id="text-33-6-1">

<p>    Stock example:
</p><dl>
<dt>Initial state</dt><dd>One P&amp;G share
</dd>
<dt>Actions available</dt><dd>Hold and sell
</dd>
<dt>Transition model</dt><dd>Hold, retain the share; sell, lose the share
</dd>
<dt>Goal test</dt><dd>P&amp;G is up 10%.
</dd>
<dt>Path cost</dt><dd>Cost of selling: dividends, &amp;c.
</dd>
</dl>


<p>
    Simple agent, given historical data, might determine that, in \(n\)
    days time, P&amp;G is up; then blindly executes \(n - 1\) holds followed
    by one buy.
</p>
<p>
    How to apply to crossfit?
</p></div>
</div>
</div>

</div>

<div id="outline-container-34" class="outline-2">
<h2 id="sec-34"><span class="section-number-2">34</span> Notes</h2>
<div class="outline-text-2" id="text-34">


</div>

<div id="outline-container-34-1" class="outline-3">
<h3 id="sec-34-1"><span class="section-number-3">34.1</span> 1</h3>
<div class="outline-text-3" id="text-34-1">

<ul>
<li>Two dimensions: thought vs. action, humanity vs. rationality.
</li>
<li>Physical simulation of a person is unnecessary for intelligence.
<ul>
<li>Mind-body dualism of Descartes?
</li>
</ul>

</li>
<li>Cognitive science brings together computer models from AI and
     experimental techniques from psychology.
</li>
<li>Real cognitive science, however, is necessarily based on
     experimental investigation of actual humans.
</li>
<li>The standard of rationality is mathematically well defined and
     completely general.
</li>
<li>We will adopt the working hypothesis that perfect rationality is a
     good starting point for analysis.
</li>
<li>Limited rationality: acting appropriately when there is not enough
     time
</li>
<li>Materialism, which holds that the brain's operation according to
     the laws of physicas constitutes the mind.
</li>
<li>Logical positivism
</li>
<li>Carnap, The Logical Structures of the World, was probably the
     first theory of mind as a computational process.
</li>
<li>Intelligence requires action as well as reasoning.
</li>
<li>Actions are justified by a logical connection between goals and
     knowledge of the action's outcome.
</li>
<li>Regression planning system
</li>
<li>The leap to a formal science required a level of mathematical
     formalization: logic, computation, probability.
</li>
<li>The world is an extremely large problem instance.
</li>
<li>Models based on satisficing&mdash;making decisions that are "good
     enough"&mdash;gave a better description of actual human behavior.
</li>
<li>Searle: brains cause minds.
</li>
<li>Behaviorism
</li>
<li>"A cognitive theory should be like a computer program."
</li>
<li>Intelligence and an artifact
</li>
<li>Parallelism&mdash;a curious convergence with the properties of the
     brain.
</li>
<li>The state of a neuron was conceived of as "factually equivalent to
     a proposition which proposed its adequate stimulus." McCulloch and
     Pitts (1943)
<ul>
<li>Neural events and the relations among them can be treated by
       means of propositional logic.
</li>
<li>For any logical expression satisfying certain conditions, one
       can find a net behaving in the fashion it describes.
</li>
<li>For every net behaving under one assumption, there exists
       another net which behaves under the other and gives the same
       results.
</li>
</ul>

</li>
<li>Perhaps "computational rationality" would have been more precise
     and less threatening, but "AI" stuck.
</li>
<li>AI from the start embraced the idea of duplicating human faculties
     such as creativity.
</li>
<li>John McCarthy referred to this period as the "Look, Ma, no hands!"
     era.
</li>
<li>"A physical symbol system has the necessary and sufficient means
     for general intelligent action."
</li>
<li>1958 . . . McCarthy define Lisp, which was to become the dominant
     AI programming language for the next 30 years.
</li>
<li>It is useful to have a formal, explicit representation of the
     world and its workings and to be able to maniplutae that
     representation with deductive processes.
</li>
<li>McCarthy, Programs with Common Sense
<ul>
<li>In this program the procedures will be described as much as
       possible in the language itself and, in particular, the
       heuristics are all so described.
</li>
<li>If one wants a machine to be able to discover an abstraction, it
       seems most likely that the machine must be able to represent
       this abstraction in some relatively simple way.
</li>
<li>The improving mechanism should be improvable.
</li>
<li>Must have or evolve concepts of partial success.
<ul>
<li>Something about ~1995 that made for a cute blog.
</li>
</ul>

</li>
<li>For example, to mest people, the number 3812 is not an object:
       they have nothing to say about it except what can be deduced
       from its structure. On the other hand, to most Americans the
       number 1776 is an object because they have filed somewhere the
       fact that it represents the year when the American Revolution
       started.
</li>
<li>One might conjecture that division in man between conscious and
       unconscious thought occurs at the boundary between
       stimulus-response heuristics which do not have to be reasoned
       about but only obeyed, and the others which have to serve as
       premises in deductions.
</li>
</ul>

</li>
<li>Machine evolution (genetic algorithms): Friedberg, 1958, 1959.
<ul>
<li>Friedberg. 1958. A learning machine Part 1. IBM Journal of
       Research and Development, 2, 2&ndash;13.
<ul>
<li>From and intent, to be sure, are related quite discontinuously
         in the compact, economical programs that programmers wrte.
</li>
</ul>

</li>
<li>Friedberg, Dunham, North. 1959. A learning machine, Part 2. IBM
       Journal of Research and Development, 3, 282&ndash;287.
</li>
</ul>

</li>
<li>Failure to come to grips with the "combinatorial explosion"
</li>
<li>The new back-propagation learning algorithms for multilayer
     netwrks that were to cause an enormous resurgence in neural-net
     research in the late 1980s were actually discovered first in 1969.
</li>
<li>Bruce Buchanan: a philosopher turned computer scientist
</li>
<li>DENDRAL was the first successful knowledge-intensive system
     (expert system).
</li>
<li>AI Winter
</li>
<li>Parallel Distributed Processing (Rumelhart, McClelland. 1986)
</li>
<li>Connectionist models: competitors to symbols models and logicist
     approach
</li>
<li>Ones that act rationally according to the laws of decision theory
     and do not try to imitate the thought steps of human experts
</li>
<li>Control theory deals with designing devices that act optimally on
     the basis of feedback from the environment.
</li>
</ul>


</div>

</div>

<div id="outline-container-34-2" class="outline-3">
<h3 id="sec-34-2"><span class="section-number-3">34.2</span> 2</h3>
<div class="outline-text-3" id="text-34-2">

<ul>
<li>Rational agents
</li>
<li>Agents behaves as well as possible (utility function?)
</li>
<li>Agent perceives its environment through sensors and acts through
     actuators.
<ul>
<li>Hands are actuators and sensors.
</li>
</ul>

</li>
<li>agent's perceptual inputs at any given instant
</li>
<li>Agent's choice depends on percept sequence to date.
</li>
<li>maps percept sequence to action.
</li>
<li>External characterization of agent (agent function): table
     mapping percept sequences to actions; internally: agent program.
</li>
<li>In a sense, all areas of engineering can be seen as designing
     artifacts that intaract with the world.
<ul>
<li>Trivializing agents to view e.g. calculators as such.
</li>
</ul>

</li>
<li>Intelligent agents, on the other hand: non-trivial decision
     making.
</li>
<li>Rational agents: does the right thing (utility).
</li>
<li>Performance measure
<ul>
<li>(This all sounds reminiscent of <a href="http://www.cs.cmu.edu/~tom/mlbook.html">Mitchell</a>, by the way.)
</li>
</ul>

</li>
<li>Sequence of actions causes the environment to go through states:
     environmental states are distinct from agent states.
<ul>
<li>Basing performance merely off of agent-states is a form of
       coherentism.
</li>
</ul>

</li>
<li>Design performance measures according to what one actually wants
     in the environment.
</li>
<li>"We leave these question as an exercise for the diligent reader."
<ul>
<li>Classic.
</li>
</ul>

</li>
<li>Rationality: performance measure, agent's prior (i.e. <i>a priori</i>)
     knowledge, agent's actions, agent's percept sequence.<sup><a class="footref" name="fnr.7" href="#fn.7">7</a></sup>
<ul>
<li>"Percept," it turns out, is the converse of "concept": "A
       Percept or Intuition is a single representation . . . a Concept
       is a collective (general or universal) representation of a
       whole class of things." (F. C. Bowen Treat. Logic)
</li>
</ul>

</li>
<li>For each percept sequence, a rational agent should select an
     action that is expected to maximize its performance measure,
     given its percept sequence and a priori knowledge.
</li>
<li>Omniscience vs. rationality
</li>
<li>Rationality maximizes <i>expected</i> performance; perfection,
     <i>actual</i> performance.
</li>
<li>Our definition of rationality does not require omniscience.
<ul>
<li>It's possible sometimes, by the way, to detect transitions in
       authorship.
</li>
</ul>

</li>
<li>Information gathering: actions in order to modify future
     percepts.
</li>
<li><i>a priori</i> rather than percepts: lacks autonomy.
</li>
<li>Ration agent: autonomous; boostrap with <i>a priori</i>, though.
</li>
<li>Just as evolution provides animals with built-in reflexes to
     survive long enough to learn for themselves
</li>
<li>Task environments
</li>
<li>Performance, Environment, Actuators, Sensors
<ul>
<li>Mitchell has: task, performance measure, training experience,
       target function, target function representation.
</li>
</ul>

</li>
<li>Fully observable vs. partially observable environment.
</li>
<li>Task environment effectively fully observable if the sensors
     detect all aspects that are <i>relevant</i> to the choice of action,
     performance measure.
</li>
<li>Single agent vs. multiagent
</li>
<li>Entity <i>may</i> vs. <i>must</i> be viewed as an agent.
</li>
<li>Competitive vs. cooperative multiagent environment
</li>
<li>Communication
</li>
<li>In some competitive environments, randomized behavior is rational
     because it avoids predictability.
</li>
<li>Deterministic vs. stochastic environment
</li>
<li>"Uncertain" environment: not fully observable or not
     deterministic
</li>
<li>Stochastic: uncertainty about outcomes quantified in terms of
     probabilities; nondeterministic: actions characterized by
     possible outcomes, no probabilities attached.
</li>
<li>Episodal vs. sequential: atomic episodes: receives percept and
     performs single action; sequential: current decision affect all
     future decisions.
</li>
<li>Static vs. dynamic: environment change while agent is
     deliberating.
</li>
<li>Discrete vs. continuous: state of the environment, time,
     percepts, actions.
</li>
<li>Known vs. unknown: "laws of physics" of the environment
</li>
<li>Hardest: partially observable, multiagent, stochastic,
     sequential, dynamic, continuous, unknown.
</li>
<li>Code repository includes environment simulator that places one or
     more agents in a simulated environment, observes their behavior
     over time, evaluates them according to a given performance
     measure.
<ul>
<li>Shit: this is something we could implement in Scheme (<a href="http://code.google.com/p/aima-java/source/checkout">java</a>,
       <a href="http://code.google.com/p/aima-python/source/checkout">python</a>, <a href="http://aima.cs.berkeley.edu/lisp/doc/overview.html">lisp</a>, <a href="http://code.google.com/p/aima-data/source/checkout">data</a>); lot of work, though? Glory?
<ul>
<li>A lot of the <a href="http://aima.cs.berkeley.edu/lisp/doc/overview-UTILITIES.html">utilities</a> are in SRFI-1; e.g. <code>transpose</code> is
         <code>zip</code>.
</li>
<li>Infinity is there.
</li>
<li>Might have to write <code>rms-error</code>; <code>ms-error</code>.
</li>
<li><code>sample-with-replacement</code>; <code>sample-without-replacement</code>
<ul>
<li>Combinatorics SRFI, anyone?
</li>
</ul>

</li>
<li><code>fuzz</code>
</li>
<li><code>print-grid</code>, &amp;c.
</li>
<li><code>the-biggest</code>, <code>the-biggest-random-tie</code>, <code>the-biggest-that</code>,
         &amp;c.
</li>
<li>Binary tree stuff
</li>
<li>Queue
</li>
<li>Heap
</li>
<li>They did CLOS-like stuff
</li>
</ul>

</li>
<li>Damn, they put some work in; could do it incrementally? Will be
       ad-hoc, I guarantee it.
</li>
<li>Maybe we can program it, given the <a href="http://aima.cs.berkeley.edu/lisp/doc/overview-AGENTS.html">agents</a> API.
<ul>
<li><code>run-environment</code> looks like something central.
</li>
</ul>

</li>
<li>What would happen if we merely translated the code to Chicken?
       Could do so, perhaps, without fully understanding it; write an
       idiomatically Scheme-port later.
</li>
<li>In that case, find some alternative to CLOS; or use tinyCLOS?
</li>
<li>Also, beginning to think that we misnamed our repo: we're
       calling it <code>aima</code>, but we'd like to write an <code>aima</code> egg; with
       <code>aima-agents</code>, <code>aima-search</code>, <code>aima-logic</code>, <code>aima-planning</code>,
       <code>aima-uncertainty</code>, <code>aima-learning</code>, <code>aima-language</code> modules.
<ul>
<li>Call it <code>aima-egg</code>? <code>aima-chicken</code>?
</li>
</ul>

</li>
<li>Translation seems like the way to go: relatively mechanical.
</li>
<li><a href="http://aima.cs.berkeley.edu/lisp/doc/overview-LOGIC.html">Incidentally</a>, "We need a new language for logical expressions,
       since we don't have all the nice characters (like upside-down
       A) that we would like to use." We can use ∀ in Scheme, can't
       we? Sure. Tough to type? Maybe. Also, think font-lock.
</li>
<li>May not be up-to-date for 3e; let's see; also, rife with
       <code>defmethod</code> and other OOisms. Can ignore it, possibly, and its
       type-checking; <code>defstructure</code> is similar, I think, to SRFI-9.
</li>
<li>Damn, they implemented unification.
</li>
<li>Not to mention: the learning stuff (e.g. decision trees).
</li>
<li>Man, should we implement this stuff ad-hoc; or otherwise depend
       on the existing implementations?
</li>
<li>Path of least resistance: do it in Allegro? Ouch.
</li>
</ul>

</li>
<li>The job of AI is to design an agent program that implements the
     agent function, the mapping from percepts to actions.
</li>
<li>Agent = architecture + program
</li>
<li>The agent program takes the current percept as input; the agent
     function, which takes the entire percept history.
</li>
<li>The agent function that the program embodies
</li>
<li>Write programs that produce rational behavior from a smallish
     program rather than a vast table.
</li>
<li>Reflex agents; model-based reflex agents; goal-based agents;
     utility-based agents.
</li>
<li>



<pre class="example">- table-driven-agent percept
  - persistent
    - percepts ()
    - table
  - append percept to percepts
  - lookup percepts table

</pre>

</li>
<li>



<pre class="example">- reflex-vacuum-agent location status
  - if dirty? status
    - suck
    - else if location = A
      - right
    - else
      - left
</pre>

</li>
<li>Simple reflex agent: select actions on the basis of the current
     precept
<ul>
<li>Learned responses, innate reflexes
</li>
<li>



<pre class="example">- simple-reflex-agent percept
  - persistent
    - rules
  - state = interpret-input(percept)
  - rule = rule-match(state rules)
  - rule.action
</pre>

</li>
<li>Works only if the correct decision can be made on the current
       percept: i.e. if the environment is fully observable.
</li>
<li>Escape from infinite loops is possible if the agent can randomize
       its actions.
</li>
<li>In single-agent environments, randomization is usually not
       rational.
</li>
<li>In most cases we can do better with more sophisticated
       deterministic agents.
</li>
</ul>

</li>
<li>Model-based reflex agents
<ul>
<li>Partial observability: keep track of the part of the world it
       can't see now.
</li>
<li>Internal state
</li>
<li>Knowledge of how world evolves independently from agent
</li>
<li>Knowledge of actions affect the world
</li>
<li>Model of the world
</li>
<li>



<pre class="example">- model-based-reflex-agent percept
  - persistent
    - state
    - model
    - rules
    - action
  - state = update-state(state action percept model)
  - rule = rule-match(state)
  - (action rule)
</pre>

</li>
<li>State of the world can contain goals.
</li>
</ul>

</li>
<li>Goal-based agents
<ul>
<li>In addition to current state, goal information that describes
       situations that are desirable
</li>
<li>Search, planning
</li>
<li>Flexible, reason about world vis à vis goals
</li>
</ul>

</li>
<li>Utility-based agents
<ul>
<li>Whereas goals are happy/unhappy, more general performance
       measure: utility
</li>
<li>Utility functional: internalization of performance measure
</li>
<li>This is not the only way to be rational: rational agent for
       vacuum has no idea what its utility function is (see exercise
       <a href="#sec-3">1.3</a>).
</li>
<li>When there are conflicting goals, utility function opts for the
       appropriate tradeoff.
</li>
<li>Partial observability, stochasticity: decision making under
       uncertainty.
</li>
<li>Expected utility of the action outcomes: derive, given the
       probabilities and utilities of each outcome.
</li>
<li>Any rational agent must behave as if it possesses a utility
       function.
</li>
<li>An agent that possesses an explicit utility function can make
       rational decisions with a general-purpose algorithm that does not
       depend on the specific utility function being maximized.
</li>
<li>Global rationality: local constraint on rational-agent designs.
</li>
<li>Choosing utility-maximizing course of action
</li>
</ul>

</li>
<li>Learning agents
<ul>
<li>Now we're getting into some Mitchell-action: critic, learning
       element, performance element, problem generator, &amp;c.
<ul>
<li>Need a book on big data?
</li>
</ul>

</li>
<li>Operate in initially unknown environments and become more
       competent.
</li>
<li>Learning element
<ul>
<li>Making improvements
</li>
</ul>

</li>
<li>Performance element
<ul>
<li>Selecting external actions
</li>
</ul>

</li>
<li>Critic
<ul>
<li>How performance element should be modified vis à vis fixed
         performance standard.
</li>
<li>Performance standard must be fixed (i.e. checkmate).
</li>
<li>Performance standard outside agent; conforms thereto.
</li>
</ul>

</li>
<li>Problem generator
<ul>
<li>Suggesting actions that will lead to new and informative
         experiences.
</li>
<li>Suboptimal actions short run, better actions long run.
</li>
</ul>

</li>
<li>Utility-based agents learning utility information
</li>
<li>Performance standard distinguishes percept as reward or
       penalty.
</li>
<li>Process of modification of each component
</li>
</ul>

</li>
<li>Atomic, factored, structured environments
<ul>
<li>Atomic
<ul>
<li>Each state of the world indivisible
</li>
</ul>

</li>
<li>Factored
<ul>
<li>Variables, attributes
</li>
</ul>

</li>
<li>Structured
<ul>
<li>Objects and relationships can be described explicitly
</li>
</ul>

</li>
<li>Increasing expressiveness
</li>
<li>Intelligent systems: operate at all points along the
       expressiveness-axis simultaneously.
</li>
</ul>

</li>
<li>Agents perceives and acts; agent function maps percept seq -&gt;
     action.
</li>
<li>Performance measure evaluates behavior.
</li>
<li>Maximize expected performance measure.
</li>
<li>Task environment: performance measure, external environment,
     actuators, sensors.
</li>
<li>Nicomachean Ethics
</li>
<li>McCarthy, Programs with Common Sense
</li>
<li>Newell and Simon, Human Problem Solving
</li>
<li>Horvitz suggests the use of rationality conceived as the
     maximization of expected utility as the basis for AI.
     Pearl, 1988.<sup><a class="footref" name="fnr.8" href="#fn.8">8</a></sup>
<ul>
<li>Horvitz, E., 1988. Reasoning Under Varying and Uncertain
       Resource Constraints, Proc. of the 7th National Conference on
       AI, Minneapolis, MN, Morgan Kauffman, pp:111-116.
</li>
<li>Horvitz, E. J., Breese, J.S., Henrion, M. 1988. Decision The-
       ory in Expert Systems and Artificial Intelligence, Journal of
       Approximate Reasoning, 2, pp247-302.
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-34-3" class="outline-3">
<h3 id="sec-34-3"><span class="section-number-3">34.3</span> 3</h3>
<div class="outline-text-3" id="text-34-3">

<ul>
<li>If environment is unknown, agent has no choice but to try
     actionsat random (see exercise 2.11).
</li>
<li>Atomic representation: states of world, wholes: no
     internalstructure visible. (What the fuck does this mean?)
</li>
<li>Informed search algorithms
</li>
<li>Goal vs. problem formulation
</li>
<li>Search, solution, execution
</li>
<li>Open-loop: executed while ignoring its precepts: breaks loop
     between agent and environment.
<ul>
<li><a href="http://en.m.wikipedia.org/wiki/Open-loop_controller">http://en.m.wikipedia.org/wiki/Open-loop_controller</a>:
<blockquote>

<p>Computes its input using only its model and the current state.
</p>
</blockquote>


</li>
</ul>

</li>
<li>(Wow: formulates plan in one pass and executes.)
</li>
<li><code>in(arad)</code>
</li>
<li><code>in(arad) -&gt; { go(Sibiu), &amp;c. }</code>
</li>
<li><code>result(s, a) -&gt; successor; result(in(arad),      go(zerind)) -&gt; in(zerind)</code>
</li>
<li>initial state, actions, transitions; graph; path
</li>
<li>given state = goal state; enumerable or abstract
</li>
<li>step cost: <code>c(s, a, s')</code>
</li>
<li>Solution: leads from initial to goal state. Optimal solution
     minimizes cost.
</li>
<li>Abstraction: removing detail from representation. (Strange that
     it's formulated negatively.)
</li>
<li>Valid: expanded to more detailed environment; useful: easier than
     original problem.
</li>
<li>Toy problems: sliding block, n-queens
</li>
<li>N-queens: no path cost; incremental vs. complete-state
</li>
<li>Knuth conjecture: factorial, square root and floor reach any
     desired positive integer.
</li>
<li>Protein design
</li>
<li>Search tree: branches are actions, nodes: states
</li>
<li>Leaf nodes, frontier
</li>
<li>Expanding nodes on frontier, until solution (or not)
</li>
<li>State space: finite; search tree: infinite (loopy)
</li>
<li>Frontier separates the state-space graph into the explored region
     and the unexplored region.
</li>
<li>Node: state, parent, action (whence), path-cost (cumulative)
<ul>
<li>(They're getting implementationy.)
</li>
<li>(Seem to be talking about breadth-first search here: there's a
       queue.)
</li>
</ul>

</li>
<li><code>solution</code> function: following parents up to root



<pre class="example">- child-node(problem, parent, action) -&gt; node
  - let
    - state (problem-result (node-state parent) action)
    - parent parent
    - action action
    - path-cose (+ (path-cost parent) (step-cost problem (node-state
      parent) action)
</pre>

</li>
<li>(Oh, never mind: queues are supersets of stacks, apparently;
     LIFOs queues being the latter. Also, priority queues.)
</li>
<li>Canonical form: bit-vector, sorted list (keys for hash-tables)
</li>
<li>Completeness, optimality, time complexity, space complexity
</li>
<li>In AI, graph represented implicitly by initial state, actions,
     transition model; frequently infinite.
</li>
<li>Complexity:
<dl>
<dt>b</dt><dd>branching factor (maximum successors)
</dd>
<dt>d</dt><dd>depth
</dd>
<dt>m</dt><dd>maximum path in state space
</dd>
</dl>

</li>
<li>(State space: finally.)
</li>
<li>Search cost
</li>
<li>Total cost: search cost + path cost
</li>
<li>Uninformed search (blind search)
</li>
<li>Order in which nodes are expanded: informed search, heuristic search
</li>
<li>Breadth-first: root and successors expanded first
<ul>
<li>FIFO queue
</li>
<li>Goal-test applied to each node when it is generated
</li>
<li>Breadth first expands the shallowest nodes
</li>
</ul>

</li>
<li>Uniform-cost search: expands \(n\) with lowest path-cost
<ul>
<li>(Greedy algorithm? Apparently not, since the goal-test is
       applied at expansion.)
</li>
<li>Frontier: priority queue ordered by \(g\)
</li>
<li>Goal-test applied when node selected for expansion; goal node
       that is generated may be on a suboptimal path
</li>
<li>Second test added, in case a better path is found to a node
       currently on the frontier. (Where is it in the algorithm?)
</li>
<li>Properties of priority queue and hash-table
</li>
<li>(There are these negative proofs.)
</li>
<li>Completeness is guaranteed, provided the cost of every step
       exceeds some small positive constant \(\epsilon\).
<ul>
<li>(\(\epsilon\) and floats)
</li>
</ul>

</li>
<li>\(C^*\): optimal value for \(C\).
</li>
<li>Uniform-cost does more work than breadth-first, expanding nodes
       even after finding a solution.
</li>
</ul>

</li>
<li>Depth-first search
<ul>
<li>Expands deepest node in current frontier.
</li>
<li>LIFO
</li>
<li>In infinite state spaces, fails if infinite non-goal path
       encountered.
</li>
<li>Depth-first search not optimal.
<ul>
<li>(Really? Interesting; depends, of course, on the order of
         nodes selected.)
</li>
</ul>

</li>
<li>\(m\) itself can be much larger than \(d\); infinite, if tree
       unbounded.
</li>
<li>Space complexity: for a graph search, no advantage; for a tree
       search: need store only a single path from root to leaf (plus
       unexplored siblings).
</li>
<li>Depth first: constraint satisfaction, propositional
       satisfiability, logic programming
<ul>
<li>(Remember, though, that kanren had some breadth-first
         facilities.)
</li>
</ul>

</li>
<li>Backtracking search: modifying current state description
</li>
</ul>

</li>
<li>Depth-limited search
<ul>
<li>Time complexity \(O(b^l)\); space complexity \(O(bl)\)
</li>
<li>Two kinds of failure: search failure, cutoff failure.
</li>
<li>Diameter of state space: any node can reach any other node in
       at most \(D\) steps
</li>
</ul>

</li>
<li>Iterative deepening (depth-first) search
<ul>
<li>Gradually increases the depth limit
</li>
<li>Like depth-first: \(O(bd)\) space complexity
</li>
<li>Like breadth-first: complete when branching factor is finite
       and optimal when the path cost is non-decreasing



<pre class="example">- iterative-deepening-search(problem) -&gt; solution or failure
  - for depth = 0 to infinity do
    - result = depth-limited-search(problem, depth)
    - if result != cutoff
      - return result
</pre>

</li>
<li>Iterative deepening, preferred uninformed search method when
       search space is large and depth unknown
</li>
<li>Do breadth-first until almost all memory consumed; iterative
       deepening on all nodes in the frontier.
</li>
<li>Iterative analog to uniform-cost search? (Overhead,
       apparently.)
</li>
</ul>

</li>
<li>Bidirectional search
<ul>
<li>Forward from the initial state and backwards from the goal:
       \(b^{d/2} + b^{d/2} &lt;&lt; b^d\)
</li>
<li>Replace goal-test to see whether frontiers intersect
</li>
<li>Check when node selected for expansion; with hash-table:
       constant time
<ul>
<li>(Hash-tables for membership testing: I like it. Hence:
         canonical form, &amp;c.)
</li>
</ul>

</li>
<li>Time, space complexity: \(O(b^{d/2})\)
</li>
<li>Reduce by half if one of the two searches done by iterative
       deepening; at least one of the frontiers must be kept in memory
</li>
<li>Requires mechanism for computing predecessors; in reversible
       case: reverse successor. Non-reversible: ingenuity
</li>
<li>Explicitly listed goal-states: construct dummy goal-states
       whose predecessors are actual goal states.
<ul>
<li>(Why?)
</li>
</ul>

</li>
<li>Informed search
<ul>
<li>Problem-specific knowledge
</li>
<li>Best-first search
<ul>
<li>Evaluation function, \(f(n)\); cost estimate
</li>
<li>Uniform-cost search, except that \(f\) instead of \(g\) orders
           the priority queue
</li>
<li>A component of \(f(n)\), the heuristic function \(h(n)\):
           estimated cost of cheapest path from the state at node \(n\)
           to a goal state.
</li>
<li>\(h(n)\), unlike \(g(n)\), depends only on the \(state\) at that
           node.
</li>
<li>If \(n\) is a goal node, \(h(n)\) = 0.
</li>
</ul>

</li>
<li>Greedy best-first search
<ul>
<li>\(f(n) = h(n)\)
</li>
<li>E.g. straight-line distance heuristic
</li>
<li>The amount of reduction depends on the particualr problem
           and on the quality of the heuristic.
</li>
</ul>

</li>
<li>\(A^*\) search (minimizing total estimated cost)
<ul>
<li>\(f(n) = h(n) + g(n) =\) estimated cost of the cheapest
           solution through \(n\).
</li>
<li>Provided that the heuristic function (\(h(n)\)) satisfies
           certain conditions, \(A^*\) is complete and optimal.
</li>
<li>Identical to uniform-cost search, except that \(A^*\) uses
           \(g + h\) instead of \(g\).
</li>
<li>Never overestimates the cost to
              reach the goal.
</li>
<li>\(h(n) \leq c(n, a, n^\prime) +
              h(n^\prime)\); triangle inequality: if there were a route
              from \(n\) to \(G_n\) via \(n^\prime\) that was cheaper than
              \(h(n)\), that would violate the property that \(h(n)\) is a
              lower bound on the cost to reach \(G_n\).
</li>
<li>Every consistent heuristic, admissible.
</li>
<li>\(h_{SLD}\): consistent heuristic; straight line between \(n\)
           and \(n^\prime\) no greater than \(c(n, a, n^\prime)\).
</li>
<li>The tree-search version of \(A^*\) is optimal if \(h(n)\) is
           admissible, while the graph-search version is optimal if
           \(h(n)\) is consistent.
</li>
<li>If \(h(n)\) is consistent, values of \(f(n)\) along any path
           are non-decreasing.
</li>
<li>\(f(n^\prime) = g(n^\prime) + h(n^\prime) = g(n) + c(n, a,
           n^\prime) + h(n^\prime) \geq g(n) + h(n) = f(n)\)
</li>
<li>There would have to be another frontier node n<sup>&prime;</sup> on
           the optimal path, by the graph selection property of 3.9.
</li>
<li>See page 77: The frontier separates the state-space graph
           into the explored region and the unexplored region, so that
           every path from the inital state to an unexplored state has
           to pass through a state in the frontier. (This is the graph
           separation property.)
</li>
<li>With an admissible but inconsistent heuristic, \(A^*\)
           requires some extra book-keeping to ensure optimality.
</li>
<li>It follows that the sequence of nodes expanded by \(A^*\) is
           in non-decreasing order of \(f(n)\). Hence, the first goal
           node selected for expansion must be an optimal solutian
           because \(f\) is the true cost for goal nodes (which have
           \(h=0\)) and all later goal nodes will be at least as
           expensive.
</li>
<li>The fact that \(f\)-costs are non-decreasing means we can
           draw contours in the state space.
</li>
<li>\(A^*\) fans out from the start node, adding nodes in
           concertric bands of increasing \(f\)-cost.
</li>
<li>With uniform-cost search (\(A^*\) using \(h(n) = 0\)), the
           bands will be circular; more accurate heuristics: bands
           will stretch toward the goal state and become more
           narrowlsy focused around the optimal path.
</li>
<li>\(A^*\) expands all nodes with \(f(n) &lt; C^*\).
</li>
<li>\(A^*\) might expand some nodes on the goal-contour before
           selecting the goal node.
</li>
<li>Completeness requires that there be only finitely many
           nodes with cost less than or equal to \(C^*\), a condition
           that is true if all step costs exceed some finite
           \(\epsilon\) and if \(b\) is finite.
<ul>
<li>(Actually (i.e. accidentally) finite, or finite in
             principle? The latter doesn't matter; but we can put
             upper bounds on \(b\) in the case of e.g. tries.
</li>
</ul>

</li>
<li>Subtree below Timisoara is pruned: because \(h_{SLD}\) is
           admissible.
</li>
<li>\(A^*\) is optimally efficient for any given consistent
           heuristic: no other optimal algorithm is guaranteed to
           expand fewer nodes than \(A^*\) (except possibly
           tie-breaking). Any algorithm that does not expand all nodes
           with \(f(n) &lt; C\) runs the risk of missing the optimal
           solution.
</li>
<li>The catch is that the number of states within the goal
           contour search space is still exponential.
</li>
<li>Absolute error, relative error
</li>
<li>Simplest model: state space, single goal, tree with
           reversible actions
</li>
<li>\(A^*\) usually runs out of space before it runs out of time
</li>
</ul>

</li>
<li>Memory-bounded heuristic search
<ul>
<li>Iterative-deepening \(A^*\) (IDA*): cutoff is the \(f\)-cost
           (\(g + h\)) rather than the depth. Practical for unit step
           costs; avoid the overhead of sorted queue of nodes.
</li>
<li>Recursive best-first search: using linear space
<ul>
<li>Modifies currnt f; unwinds along the recursion
</li>
<li>Somewhat more efficient than IDA*, suffers from excessive
             node regeneration
</li>
<li>Space complexity: linear; time-complexity: depends on
             heuristic and how often best path changes.
</li>
</ul>

</li>
<li>Memory-bounded A*
<ul>
<li>Drop oldest worst leaf when memory full
</li>
<li>SMA* is a fairly robust choice for finding optimal
             solutions, particularly when the state space is a graph.
</li>
</ul>

</li>
</ul>

</li>
<li>Metalevel state-space
<ul>
<li>Goal of learning is to minimize total cost of problem solving.
</li>
</ul>

</li>
</ul>

</li>
</ul>

</li>
<li>8-puzzle: exhaustive tree search: 3<sup>22</sup>; graph search: 181,440
     distinct states.
</li>
<li>Manhattan distance
</li>
<li>Quality of heuristic: effective branching factor \(b^*\). (If the
     total nuber of nodes generated by \(A^*\) for a particualr problem
     is \(N\) and the solution depth is \(d\), then \(b^*\) is the branching
     factor that a uniform tree of depth \(d\) would have to have in
     order to contain \(N + 1\) nodes.)
</li>
<li>Experimental measurements of \(b^*\) on a small set of problems can
     provide a good uide to the heuristic's overall usefulness
</li>
<li>A well-designed heuristic would have a value of \(b^*\) close to
     \(1\).
</li>
<li>"\(h_2\) dominates \(h_1\)."
</li>
<li>Problem with fewer restrictions on the actions is called a
     relaxed problem: whence heuristic. The state-space of the relaxed
     problem is a supergraph of the original state space.
</li>
<li>The relaxed problem may have better solutions of the added edges
     provide short cuts.
</li>
<li>The cost of an optimal solution to a relaxed problem is an
     admissible heuristic for the original problem.
</li>
<li>It must obey the triangle inequality and is therefore consistent.
</li>
<li>Relaxed problems solved without search, otherwise: too expensive.
</li>
<li><code>Absolver</code>: Prieditis
</li>
<li>\(h(n) = max{h_1(n), ... , h_m(n)}\)
</li>
<li>Composite heuristic: \(h\) is admissible; consistent, dominates
     component heuristics.
</li>
<li>Pattern databases
</li>
<li>Solutions share moves, cannot be additively combined
</li>
<li>Disjoint pattern databases
</li>
<li>Nonadditive heuristic
</li>
<li>Inductive methods supplied with features of a state that are
     relevant to predicting the state's value.
</li>
<li>Goal identified, well-defined problem formulated.
</li>
<li>Initial state, actions, transition model, goal test, path cost.
     This is the state space. Path through state space to goal state:
     solution.
</li>
<li>States and actions: atomic; don't consider internal structure.
     (What the fuck does this mean: features?)
</li>
<li>Tree-search: all paths; graph-search: avoids redundant paths.
</li>
<li>Completeness, optimality, time complexity, space complexity.
</li>
<li>Uninformed search
<ul>
<li>Breadth-first
</li>
<li>Uniform-cost
</li>
<li>Depth-first
</li>
<li>Depth-limited
</li>
<li>Iterative deepening
</li>
<li>Bidirectional
</li>
</ul>

</li>
<li>Informed search
<ul>
<li>Heuristic function
</li>
<li>Best-first search
</li>
<li>Greedy best-first search
</li>
<li>\(A^*\) search
</li>
<li>RBFS
</li>
<li>SMA*
</li>
</ul>

</li>
<li>Dynamic programming can be seen as a form of depth-first search
     on graphs.
</li>
</ul>

</div>

</div>

<div id="outline-container-34-4" class="outline-3">
<h3 id="sec-34-4"><span class="section-number-3">34.4</span> Lectures</h3>
<div class="outline-text-3" id="text-34-4">


</div>

<div id="outline-container-34-4-1" class="outline-4">
<h4 id="sec-34-4-1"><span class="section-number-4">34.4.1</span> 1</h4>
<div class="outline-text-4" id="text-34-4-1">

<ul>
<li>AI: mapping from sensors to actuators
<ul>
<li>Voice, child-like engagement
</li>
</ul>

</li>
<li>Fully vs. partially observable
</li>
<li>Deterministic vs. stochastic
</li>
<li>Discrete vs. continuous
</li>
<li>Benign vs. adversarial
</li>
<li>Uncertainty management
</li>
</ul>

</div>

</div>

<div id="outline-container-34-4-2" class="outline-4">
<h4 id="sec-34-4-2"><span class="section-number-4">34.4.2</span> 2</h4>
<div class="outline-text-4" id="text-34-4-2">

<ul>
<li>Initial state
</li>
<li>\(\texttt{actions}(state) \to {a_1, a_2, a_3, \dots}\)
</li>
<li>\(\texttt{result}(state, action) \to state^\prime\)
</li>
<li>\(\texttt{goal-test}(state) \to T|F\)
</li>
<li>\(\texttt{path-cost}(state \xrightarrow{action} state \xrightarrow{action} state) \to n\)
</li>
<li>\(\texttt{step-cost}(state, action, state^\prime) \to n\)
</li>
<li>Navigate the state space by applying actions
</li>
<li>Separate state into three parts: ends of paths (frontier);
      explored and unexplored regions.
</li>
<li>Step-cost
</li>
<li>Tree-search
<ul>
<li>Family-resemlance; difference: which path to look at first.
</li>
</ul>

</li>
<li>Depth-first search: shortest-first search
</li>
</ul>

</div>
</div>

</div>

<div id="outline-container-34-5" class="outline-3">
<h3 id="sec-34-5"><span class="section-number-3">34.5</span> Turing, Computing Machinery and Intelligence</h3>
<div class="outline-text-3" id="text-34-5">

<ul>
<li>Can machines think?
</li>
<li>It is A's object in the game to try and cause C to make the wrong
     identification.
<ul>
<li>Didn't realize there was an adversarial element to the Turing
       test.
</li>
</ul>

</li>
<li>What will happen when a machine takes the part of A in this game?
</li>
<li>. . . drawing a fairly sharp line between the physical and the
     intellectual capacities of man.
<ul>
<li>A reasonable dualism
</li>
</ul>

</li>
<li>May not machines carry out something which ought to be described
     as thinking but which is very different from what a man does?
<ul>
<li>The humanity/rationality plane of AI?
</li>
</ul>

</li>
<li>Imitation game
<ul>
<li>Simulacrum sufficeth
</li>
</ul>

</li>
<li>It is probably possible to rear a complete individual from a
     single cell of the skin (say) of a man . . . but we would not be
     inclined to regard it as a case of "constructing a thinking
     machine".
</li>
<li>Digital computer:
<ol>
<li>Store
</li>
<li>Executive unit
</li>
<li>Control
</li>
</ol>

</li>
<li>It is not normally possible to determine from observing a machine
     whether it has a random element, for a similar effect can be
     produced by such devices as making the choices depend on the
     digits of the decimal for \(\pi\).
</li>
<li>Discrete state machines: strictly speaking there are no such
     machines. Everything really moves continuously.
</li>
<li>This is reminiscent of Laplace's view that from the complete
     state of the universe at one moment of time, as described by the
     positions and velocities of all particles, it should be possible
     to predict all future states.
</li>
<li>This special property of digital computers, that they can mimic
     any discrete state machine, is described by saying that they are
     universal machines.
</li>
<li>"Are there imaginable digital computers which would do well in
     the imitation game?" \(\to\) "Are there discrete state machines
     which would do well?"
</li>
<li>I believe that in about fifty years' time it will be possible to
     programme computers, with a storage capacity of about \(10^9\), to
     make them play the imitation game so well that an average
     interrogator will not have more than \(70\%\) chance of making the
     right identification after five minutes of questioning.
<ul>
<li>Russell/Norvig, 12: storage units: \(10^15\)
</li>
<li><a href="http://en.wikipedia.org/wiki/Loebner_Prize#2008">Loebner prize</a>:
<blockquote>

<p>Elbot of Artificial Solutions won the 2008 Loebner Prize bronze
award, for most human-like artificial conversational entity,
through fooling three of the twelve judges who interrogated it
(in the human-parallel comparisons) into believing it was
human. This is coming very close to the \(30\%\) traditionally
required to consider that a program has actually passed the
Turing test.
</p>
</blockquote>


<ul>
<li>From a <a href="http://technology.timesonline.co.uk/tol/news/tech_and_web/article4934858.ece">judge</a>:
<blockquote>

<p>He predicted that by the end of the century, computers would
have a 30 per cent chance of being mistaken for a human being
in five minutes of text-based conversation.
</p>
</blockquote>


<p>
         I thought this was mistaken (should be \(70\)), but it is
         indeed correct.
</p></li>
</ul>

</li>
<li>In other words, a damn-good guess.
</li>
</ul>

</li>
<li>Conjectures are of great importance since they suggest useful
     lines of research.
</li>
<li>We might expect that He would only exercise this power in
     conjunction with a mutation which provided the elephant with an
     appropriately improved brain to minister to the needs of this
     soul.
</li>
<li>We like to believe that Man is in some subtle way superior to the
     rest of creation.
</li>
<li>"The consequences of machines thinking would be too dreadful." I
     do not think that this argument sufficiently substantial to
     require refutation. Consolation would would be more appropriate:
     perhaps this should be sought the transmigration of souls.
</li>
<li>There are limitations to the powers of discrete-state
     machines. The best known of these results is known as Gödel's
     theorem, and shows that in any sufficiently powerful logical
     system statements can be formulated which can neither be proved
     nor disproved within the system, unless possibly the system
     itself is inconsistent.
</li>
<li>"Will this machine every answer 'Yes' to any question?" It can be
     shown that the answer is either wrong or not forthcoming.
</li>
<li>The only way to know that a man thinks is to be that particular
     man. It is in fact the solipsist point of view.
</li>
<li>I do not wish to give the impression that I think there is no
     mystery about consciousness. There is, for instance, something of
     a paradox connected with any attempt to localise it.
</li>
<li>When a burnt child fears the fire and shows that he fears it by
     avoiding it, I should say that he was applying scientific
     induction.
</li>
<li>It would deliberately introduce mistakes in a manner calculated
     to confuse the interrogator.
</li>
<li>By observing the results of its own behaviour it can modify its
     own programmes so as to achieve some purpose more effectively.
</li>
<li>This is the assumption that as soon as a fact is presented to a
     mind all consequences of that fact spring into the mind
     simultaneously with it.
</li>
<li>The undistributed middle is glaring.
</li>
<li>I would defy anyone to learn from these replies sufficint about
     the programme to be able to predict any replies to untried
     values.
</li>
<li>A smallish proportion are super-critical. An idea presented to
     such a mind may give rise to a whole "theory" consisting of
     secondary, tertiary and more remote ideas.
<ul>
<li>Sponteneity
</li>
</ul>

</li>
<li>These last two paragraphs should be described as "recitations
     tending to produce belief."
</li>
<li>The only satisfactory support that can be given will be that
     provided by waiting for the end of the century and then doing the
     experiment described.
</li>
<li>Estimates for the storage capacity of the brain vary from \(10^10\)
     to \(10^15\) binary digits.
<ul>
<li>Russell/Norvig (12): \(10^13\) synapses
</li>
</ul>

</li>
<li>At my present rate of working I produce about a thousand digits
     of programme a day, so that about sixty workers, working steadily
     through the fifty years might accomplish the job, if nothing went
     into the waste-paper basket.
<ul>
<li>Mythical man-month?
</li>
</ul>

</li>
<li>The child-programme and the education process
</li>
<li>One might have a complete system of logical inference "built
     in". The store would be largely occupied with definitions and
     propositions. Certain propositions may be described as
     "imperatives". As soon as an imperative is classed as
     "well-established" the appropriate action takes place.
<ul>
<li>Compare McCarthy, Programs with Common Sense, regarding
       imperatives.
</li>
</ul>

</li>
<li>These choices make the difference between a brilliant and a
     footling reasoner.
</li>
<li>We can only see a short distance ahead, but we can see plenty
     there that needs to be done.
</li>
</ul>


</div>
</div>

</div>

<div id="outline-container-35" class="outline-2">
<h2 id="sec-35"><span class="section-number-2">35</span> TODOs</h2>
<div class="outline-text-2" id="text-35">


</div>

<div id="outline-container-35-1" class="outline-3">
<h3 id="sec-35-1"><span class="section-number-3">35.1</span> <span class="todo TODO">TODO</span> Focus on one or two problems for the coming week.</h3>
<div class="outline-text-3" id="text-35-1">

<p>   Assign people to be responsible for them.
</p></div>

</div>

<div id="outline-container-35-2" class="outline-3">
<h3 id="sec-35-2"><span class="section-number-3">35.2</span> <span class="todo TODO">TODO</span> Modify 2.11 to list available actions?</h3>
<div class="outline-text-3" id="text-35-2">

<p>   No: that changes the problem.
</p></div>

</div>

<div id="outline-container-35-3" class="outline-3">
<h3 id="sec-35-3"><span class="section-number-3">35.3</span> <span class="todo TODO">TODO</span> A listing environment</h3>
<div class="outline-text-3" id="text-35-3">

</div>

</div>

<div id="outline-container-35-4" class="outline-3">
<h3 id="sec-35-4"><span class="section-number-3">35.4</span> <span class="todo TODO">TODO</span> Blog about barabási-albert vs. depth-first graph-creation.</h3>
<div class="outline-text-3" id="text-35-4">

<p>   The former produces nice hubs and cycles, but the problem of
   aligning complementary directions (like graph-coloring?) is
   NP-complete. Resorting to depth-first with no cycles; would have to
   have an exponential adjunct for cycles.
</p>
<p>
   Way to quantify the difference between the two with respect to e.g.
   average degree of nodes?
</p></div>

</div>

<div id="outline-container-35-5" class="outline-3">
<h3 id="sec-35-5"><span class="section-number-3">35.5</span> <span class="done DONE">DONE</span> 2.11</h3>
<div class="outline-text-3" id="text-35-5">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:57</span></span><br/>
</p>
</div>

<div id="outline-container-35-5-1" class="outline-4">
<h4 id="sec-35-5-1"><span class="section-number-4">35.5.1</span> <span class="done DONE">DONE</span> Graph world</h4>
<div class="outline-text-4" id="text-35-5-1">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:44</span></span><br/>
</p>


<pre class="example">(use aima-vacuum
     debug
     srfi-25)


</pre>



<pre class="example">┌─┬┐
│ ││
└─┴┘
</pre>


<p>
    If we get too fancy, it's going to take us a while; why not just
    draw a couple random barriers in a box? Not enough to probably
    block access, &amp;c.
</p>
<p>
    The world should be a graph; we'll search on that graph and thereby
    anticipate chapter 3. Debug output can produce an animated graph
    over graphviz (see e.g. the Iraq project). Show how the agent moves
    and the squares clean?
</p>
<p>
    If we want to create a random graph, but not necessarily a maze
    that fills up a given extent, we can specify e.g. the number of
    nodes. We have to have some condition for backtracking, though:
    random? The fill-up-an-extent maze-algorithm backtracks when it has
    no unseen neighbors; I suppose we can weight backtrack vs. branch
    differently and see what happens. I wonder whether e.g. 50/50 will
    produce a graph with a lot of shallow paths and high branching.
    Make backtracking and branching relatively expensive, then.
</p>
<p>
    The random reflex agent might do well in a completely connected
    graph, incidentally, since it doesn't have to make any decisions
    about where to go; it might fair poorly in a linear environment,
    since at least half of the time it will be unable to move.
</p>
<p>
    It turns out that <a href="http://en.wikipedia.org/wiki/Random_graph">random graphs</a> are an open topic of research; with
    the <a href="http://en.wikipedia.org/wiki/Erdős–Rényi_model">Erdős–Rényi</a>, <a href="#Watts-Strogatz">Watts-Strogatz</a>, <a href="http://en.wikipedia.org/wiki/Barabási–Albert_model">Barabási–Albert</a> models. Let's go
    with Barabási–Albert: it looks relatively easy to implement and has
    interesting clustering properties (power-law degree-distributions).
</p>
<p>
    Confer <a href="http://igraph.sourceforge.net/doc/R/ba.game.html">Gabor Csardi's implementation of Barabási-Albert</a>, which has
    zero-appeal and power-of-preferential-attachment parameters; it
    also begins with one node. It can dispense with the two nodes from
    WP because of the zero-appeal parameter. It also does directed and
    undirected; and distinguishes, somehow, between in- and both
    in-and-out-degrees. It also does not divide by the total number of
    nodes (or is it the total number of edges?).
</p>
<p>
    (Holy shit: <a href="http://www.drdobbs.com/architecture-and-design/simulating-small-world-networks/184405611">this chick</a> implements all three in Perl; she claims
    that you divide by all the degrees in the graph, which is
    consistent with WP's notation. With Gabor Csardi, she also has a
    parameter \(m\) of edges to add at each time; her graph also starts
    with no edges, but has \(m_0\) nodes at \(t_0\); they both discuss the
    problem of multiple edges: it may not suffice to merely not
    generate them, you should detect and ignore them. No, wait; she's
    talking about something different: "To keep the code simple, I
    don't test whether the \(m\) nodes that are connecting to the new
    node are all different." Gabor, on other hand, talks about an
    algorithm which "never generates multiple edges." Wait again: she's
    talking about the same thing. Should we ignore duplicate links and
    add another; or just refrain from duplicating? Damn.)
</p>
<p>
    It sounds like we can create a connected-graph if we start with the
    seed-graph-of-degree-one hack; using zero-appeal, we might have
    disconnected but dirty squares. Disconnected but dirty squares are
    romantic, however; and their effect should fall prey to the law of
    large numbers if we repeat it enough times.
</p>
<p>
    We are, of course, constrained by the relative directions up, down,
    left and right; the degree of any node is therefore \(\leq 4\). In
    that sense, maybe the matrix is a reasonable representation?
    Barabási-Albert isn't going to be able to create hubs with that
    sort of upper bound. Push forward anyway, or capitulate to grid?
</p>
<p>
    I like the idea of a graph, each of whose nodes contains \(0 \leq
    neighbors \leq 4\); which are assigned the arbitrary designations
    <code>up</code>, <code>down</code>, <code>left</code> and <code>right</code>. The only problem is that, for
    experimentation, we're going to have to generate a complete
    grid-like world. That is a pain in the ass.
</p>
<p>
    I take that back: can we create something like a torus or a sphere
    where the grid wraps around; that is, where every square is totally
    connected (edges being connected to the opposite edge)? A random
    reflex-vacuum might be able to do well there.
</p>
<p>
    If with modified Barabási-Albert we're going to have disconnected
    nodes, by the way, if e.g. \(p_i\) is false for every node; or are we
    guaranteed to connect \(m\) nodes every time? See <a href="http://eaton.math.rpi.edu/CSUMS/Papers/ScaleFree/Scale-Free%20Networks.pdf">e.g.</a>:
</p>
<blockquote>

<p>The generalized creation process of a scale-free-network is defined
by Barabasi and Albert [BaBo03]: Start with a small number of nodes
\(m0\). During each step, insert a new node with connections to \(m
\leq m0\) existing nodes. The probability \(P_v\) of node \(v\) being
connected to a new node depends on its degree \(j_v\). The higher its
degree, the greater the probability that it will receive new
connections ("the rich get richer").
</p>
</blockquote>


<p>
    Since there's an <a href="http://www.bsse.ethz.ch/cobi/education/Math_Mod_basic/ex8_2011.pdf">upper bound</a> on the dividend, we're going to
    degenerate into a network whose edges are picked at random; and,
    worse yet, might end up with many unconnected nodes.
</p>
<p>
    Can't we just create the world initially, by the way, instead of
    creating the graph and then the world? Interestingly, the relative
    directions won't meet up: <code>up</code> is paired with <code>left</code>, &amp;c. Is such a
    world physically possible? Yes, if the relative directions for each
    location are different; they're relative, after all. It shouldn't
    matter to the agents.
</p>
<p>
    It's a weird world, though.
</p>
<p>
    I want to have an environment that spits out an animated graph of
    where the agent is (green), which cells are dirty (grey), which are
    clean (white).
</p>



<pre class="example">(use aima
     aima-vacuum
     debug
     extras
     files
     lolevel
     posix
     random-bsd
     srfi-1
     srfi-69)

(define (connect! graph connectend connector)
  (hash-table-update!/default
   graph
   connectend
   (lambda (neighbors)
     (lset-adjoin eq? neighbors connector))
   '()))

(define (biconnect! graph connectend connector)
  (connect! graph connectend connector)
  (connect! graph connector connectend))

(define make-node gensym)

(define (sum-degrees graph)
  (hash-table-fold graph
                   (lambda (from to sum) (+ (length to) sum))
                   0))

(define (write-graph-as-dot graph)
  (let ((graph (hash-table-copy graph)))
    (display "digraph G {")
    (for-each (lambda (from) (format #t "~a;" from))
      (hash-table-keys graph))
    (hash-table-walk
     graph
     (lambda (from to)
       (for-each
           (lambda (to) (format #t "~a-&gt;~a;" from to)) to)))
    (display "}\n")))

(define (write-graph-as-pdf graph)
  (receive (input output id)
    (process "neato -Tpdf")
    (with-output-to-port output
      (lambda () (write-graph-as-dot graph)))
    (close-output-port output)
    (with-input-from-port input
      (lambda ()
        (do ((line (read-line) (read-line)))
            ((eof-object? line))
          (write-line line))))
    (close-input-port input)))

(define (display-pdf pdf)
  (system* "evince -s ~a" pdf))

;;; Number of nodes to try and join to the seed.
(define default-n-nodes (make-parameter 1000))

;;; Number of edges to attempt to create per step.
(define default-m (make-parameter 4))

;;; We're going to have to maintain the e.g. clean-dirty data in
;;; separate structures if the graph merely expresses node
;;; relationships.
;;;
;;; That's fine: let's maintain a second hash-table for `node -&gt;
;;; clean?'; so-called obstacles are merely lack of passage.
;;;
;;; We need a mapping from nodes to up, down, left, right; however.
;;; Can we assign this randomly, somehow? Otherwise, we'll get an
;;; up-biased graph; or, rather: an up-, down-, left-, right-biased
;;; graph (in that order).
;;;
;;; Yet another hash-table that assigns direction?
;;;
;;; No: one hash table that expresses the graph structure, another
;;; that maps symbols to objects; which objects contain: status,
;;; relative direction. Problem is: the same node can be linked to
;;; more than once from different relative directions (the graph has
;;; loops; doesn't it? Yes it does: but they're relatively rare. In
;;; this sense: Barabási-Albert graphs are probably different from
;;; grids with sparse obstactles).
;;;
;;; Fuck it: let's have an up-biased graph; maybe the reflex agent can
;;; use that knowledge to its advantage.
;;;
;;; Should we allow an optional seed here, so we can reproduce the
;;; input?
(define barabási-albert-graph
  (case-lambda
   (() (barabási-albert-graph (default-n-nodes) (default-m)))
   ((n-nodes m)
    (let ((graph (make-hash-table)))
      (biconnect! graph (make-node) (make-node))
      (do ((n-nodes n-nodes (- n-nodes 1)))
          ((zero? n-nodes))
        (let ((new-node (make-node)))
          (do ((from (hash-table-keys graph) (cdr from))
               (m m (- m 1)))
              ((or (null? from) (zero? m)))
            (let ((sum-degrees (sum-degrees graph)))
              (let* ((from (car from))
                     (degrees-from (length (hash-table-ref graph from))))
                (if (and (&lt; degrees-from 4)
                         (&lt; (random-real) (/ degrees-from sum-degrees)))
                    (biconnect! graph from new-node)))))))
      graph))))

;; (with-output-to-file "graph.pdf"
;;   (lambda () (write-graph-as-pdf (barabási-albert-graph))))

;; (display-pdf "graph.pdf")

(define-record no-passage)
(define no-passage (make-no-passage))
(define passage? (complement no-passage?))

(define up 2)
(define up? (cute = &lt;&gt; 2))

(define down 3)
(define down? (cute = &lt;&gt; 3))

(define-record location
  status
  left
  right
  up
  down)

(define-record-printer location
  (lambda (location output)
    (display (record-&gt;vector location) output)))

(define default-width (make-parameter 1600))

(define default-height (make-parameter 900))

;;; Height and width are in pixels.
(define write-dot-preamble
  (case-lambda
   ((agent step)
    (write-dot-preamble agent step (default-height) (default-width)))
   ((agent step height width)
    (display "digraph G {")
    (display "node [style=filled, fontname=monospace];")
    (display "edge [fontname=monospace];")
    (if (and height width)
        (begin
          (display "graph [fontsize=48.0, ratio=fill];")
          ;; Phew: viewports are specified in points at 72 per inch;
          ;; size is specified in pixels at 96 per inch.
          (let ((width-in-inches (/ width 96))
                (height-in-inches (/ height 96)))
            (format #t "graph [viewport=\"~a,~a\", size=\"~a,~a!\"];"
                    (* width-in-inches 72)
                    (* height-in-inches 72)
                    width-in-inches
                    height-in-inches))))
    (if step
        (format #t "graph [label=\"Score: ~a; step: ~a\"]"
                (agent-score agent)
                step)))))

(define (write-dot-nodes world agent)
  (hash-table-walk
   world
   (lambda (name location)
     (let ((color
            (cond ((eq? (agent-location agent) name) "green")
                  ((clean? (location-status location)) "white")
                  (else "gray"))))
       (format #t "~a [fillcolor=~a];" name color)))))

(define (write-dot-edges world)
  (hash-table-walk
   world
   (lambda (name location)
     (let ((left (location-left location))
           (right (location-right location))
           (up (location-up location))
           (down (location-down location)))
       (if (passage? left)
           (format #t "~a-&gt;~a [label=l];" name left))
       (if (passage? right)
           (format #t "~a-&gt;~a [label=r];" name right))
       (if (passage? up)
           (format #t "~a-&gt;~a [label=u];" name up))
       (if (passage? down)
           (format #t "~a-&gt;~a [label=d];" name down))))))

(define (write-dot-postscript) (display "}\n"))

(define write-world-as-dot
  (case-lambda
   ((world agent) (write-world-as-dot world agent #f))
   ((world agent step)
    (write-world-as-dot world agent step (default-height) (default-width)))
   ((world agent step height width)
    (write-dot-preamble agent step height width)
    (write-dot-nodes world agent)
    (write-dot-edges world)
    (write-dot-postscript))))

(define (write-world-as-pdf world agent pdf)
  (receive (input output id)
    (process "neato" `("-Tpdf" "-o" ,pdf))
    (with-output-to-port output
      ;; Do we really need a blank label, for some reason?
      (lambda () (write-world-as-dot world agent #f #f #f)))
    (flush-output output)
    (close-output-port output)
    (close-input-port input)))

(define (write-world-as-gif world agent frame gif)
  (receive (input output id)
    (process "neato" `("-Tgif" "-o" ,gif))
    (with-output-to-port output
      (lambda () (write-world-as-dot world agent frame)))
    (flush-output output)
    (close-output-port output)
    (close-input-port input)))

(define (graph-&gt;world graph)
  (let ((world (make-hash-table)))
    (hash-table-walk
     graph
     (lambda (from to)       
       (let* ((rest (make-list (- 4 (length to)) no-passage))
              (to (append to rest))
              (to (shuffle to (lambda (x) (random (length to))))))
         (let ((location
                (make-location
                 dirty
                 (list-ref to left)
                 (list-ref to right)
                 (list-ref to up)
                 (list-ref to down))))             
           (hash-table-set! world from location)))))
    world))

(define (random-start world)
  (let ((locations (hash-table-keys world)))
    (list-ref locations (random-integer (length locations)))))

(define (make-randomized-reflex-graph-agent start)
  (make-reflex-agent
   start
   (lambda (location clean?)
     (if clean?
         (list-ref '(left right up down) (random 4))
         'suck))))

(define (make-graph-environment world agent)
  (lambda ()
    (let* ((location (agent-location agent))
           (action ((agent-program agent)
                    location
                    (clean? (location-status
                             (hash-table-ref world location))))))
      (debug-print "agent-action" action)
      (case action
        ((left)
         (let ((left (location-left (hash-table-ref world location))))
           (if (passage? left)
               (agent-location-set! agent left))))
        ((right)
         (let ((right (location-right (hash-table-ref world location))))
           (if (passage? right)
               (agent-location-set! agent right))))
        ((up)
         (let ((up (location-up (hash-table-ref world location))))
           (if (passage? up)
               (agent-location-set! agent up))))
        ((down)
         (let ((down (location-down (hash-table-ref world location))))
           (if (passage? down)
               (agent-location-set! agent down))))
        ((noop))
        ((suck)
         (location-status-set! (hash-table-ref world location) clean))
        (else (error "graph-environment -- Unknown action"))))))

(define (make-graph-world) (graph-&gt;world (barabási-albert-graph)))

(define (make-graph-performance-measure world agent)
  (lambda ()
    (let ((clean-locations
           ;; Quicker with map and apply?
           (hash-table-fold
            world
            (lambda (name location clean-locations)
              (if (clean? (location-status location))
                  (+ clean-locations 1)
                  clean-locations))
            0)))
      (agent-score-set! agent (+ (agent-score agent) clean-locations)))))

(define (make-graph-animating-environment world agent directory)
  (let ((frame 0))
    (lambda ()
      (let ((gif (make-pathname directory (number-&gt;string frame) "gif")))
        (write-world-as-gif world agent frame gif))
      (set! frame (+ frame 1)))))

;;; I think make-step-limited-environment is a special case of
;;; make-finalizing-environment with noop.
;;;
;;; This probably belongs in aima; should we preserve the
;;; step-limited-environment as a specialization of this?
(define (make-finalizing-environment finalizer final-step)
  (let ((step 0))
    (lambda ()
      (set! step (+ step 1))
      (let ((continue? (&lt; step final-step)))
        (if (not continue?) (finalizer))
        continue?))))

(define (make-animation-finalizer directory)
  (lambda ()
    (system "rm -fv graph.gif")
    (system* "convert $(find ~a -type f | sort -k 1.~a -n) -loop 0 graph.gif"
             directory
             (+ (string-length directory) 2))
    (system* "identify ~a/0.gif" directory)
    (system "mencoder graph.gif -ovc lavc -o graph.avi")))

(define (make-stateful-graph-agent start)
  (make-reflex-agent
   start
   (let ((world (make-hash-table)))
     (lambda (location clean?)
       'right))))

(let* ((world (make-graph-world))
       (start (random-start world))
       (agent (make-stateful-graph-agent start)))
  (let ((directory (create-temporary-directory))
        (steps 20))
    (write-world-as-pdf world agent "world.pdf")
    (display-pdf "world.pdf")
    #;
    (simulate
     (compose-environments
      (make-step-limited-environment 10)
      (make-graph-environment world agent)
      (make-debug-environment agent)
      (make-graph-performance-measure world agent)
      ;; Can't this contain its finalizer? Maybe even give it the
      ;; terminal frame?
      ;; (make-graph-animating-environment world agent directory)
      ;; (make-finalizing-environment
      ;;  (make-animation-finalizer directory)
      ;;  steps)
      ))))

</pre>


<p>
    This is a cool graph with a large ring, incidentally; I don't know
    what the random seed was, unfortunately:
</p>



<pre class="example">digraph G {
  node [style=filled, fontname=monospace]
  edge[fontname=monospace]
  g17263 [fillcolor=white]
  g17270 [fillcolor=white]
  g17273 [fillcolor=white]
  g17278 [fillcolor=white]
  g17298 [fillcolor=white]
  g17310 [fillcolor=gray]
  g17316 [fillcolor=white]
  g17329 [fillcolor=gray]
  g17336 [fillcolor=gray]
  g17357 [fillcolor=white]
  g17226 [fillcolor=white]
  g17227 [fillcolor=gray]
  g17229 [fillcolor=white]
  g17231 [fillcolor=gray]
  g17232 [fillcolor=green]
  g17234 [fillcolor=gray]
  g17235 [fillcolor=gray]
  g17237 [fillcolor=gray]
  g17238 [fillcolor=white]
  g17239 [fillcolor=gray]
  g17245 [fillcolor=white]
  g17258 [fillcolor=white]
  g17259 [fillcolor=gray]
  g17262 [fillcolor=gray]
  g17263-&gt;g17258 [label=d]
  g17270-&gt;g17262 [label=l]
  g17273-&gt;g17245 [label=u]
  g17278-&gt;g17245 [label=l]
  g17298-&gt;g17262 [label=u]
  g17310-&gt;g17258 [label=l]
  g17310-&gt;g17262 [label=u]
  g17316-&gt;g17259 [label=l]
  g17329-&gt;g17258 [label=u]
  g17336-&gt;g17259 [label=u]
  g17357-&gt;g17259 [label=u]
  g17226-&gt;g17229 [label=l]
  g17226-&gt;g17227 [label=r]
  g17227-&gt;g17226 [label=d]
  g17229-&gt;g17226 [label=l]
  g17229-&gt;g17234 [label=r]
  g17229-&gt;g17232 [label=u]
  g17229-&gt;g17231 [label=d]
  g17231-&gt;g17229 [label=l]
  g17232-&gt;g17229 [label=l]
  g17232-&gt;g17238 [label=u]
  g17234-&gt;g17235 [label=l]
  g17234-&gt;g17229 [label=r]
  g17234-&gt;g17239 [label=u]
  g17234-&gt;g17237 [label=d]
  g17235-&gt;g17234 [label=u]
  g17237-&gt;g17245 [label=l]
  g17237-&gt;g17234 [label=r]
  g17237-&gt;g17239 [label=u]
  g17238-&gt;g17259 [label=l]
  g17238-&gt;g17258 [label=u]
  g17238-&gt;g17232 [label=d]
  g17239-&gt;g17234 [label=l]
  g17239-&gt;g17237 [label=r]
  g17245-&gt;g17278 [label=l]
  g17245-&gt;g17273 [label=r]
  g17245-&gt;g17262 [label=u]
  g17245-&gt;g17237 [label=d]
  g17258-&gt;g17310 [label=l]
  g17258-&gt;g17263 [label=r]
  g17258-&gt;g17329 [label=u]
  g17258-&gt;g17238 [label=d]
  g17259-&gt;g17357 [label=l]
  g17259-&gt;g17336 [label=r]
  g17259-&gt;g17316 [label=u]
  g17259-&gt;g17238 [label=d]
  g17262-&gt;g17245 [label=l]
  g17262-&gt;g17310 [label=r]
  g17262-&gt;g17298 [label=u]
  g17262-&gt;g17270 [label=d]
}
</pre>


<p>
    Converting the <code>gif</code> output to an <code>avi</code> à la <code>mencoder</code>:
</p>



<pre class="example">mencoder graph.gif -ovc lavc -o graph.avi
</pre>

</div>

</div>

<div id="outline-container-35-5-2" class="outline-4">
<h4 id="sec-35-5-2"><span class="section-number-4">35.5.2</span> <span class="done DONE">DONE</span> Depth-first graph constructor</h4>
<div class="outline-text-4" id="text-35-5-2">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:44</span></span><br/>
    Like Barabási-Albert, the probability of branching is relative to
    the degree of the current node (plus some probability for
    degreeless nodes); otherwise, follow a random branch; otherwise,
    backtrack.
</p>



<pre class="example">- make-seed-world
  - let
    - world make-hash-table
    - root make-node
    - child make-node
      - biconnect! world root child (random 4)
      - values world root
- # Let's recreate location-left{,-set!}, &amp;c.
- make-preferential-depth-first-world n-nodes
  - receive world root
    - make-seed-world
    - let iter
      - node root
      - n-nodes n-nodes
      - n-degrees 0
        - if zero? n-nodes
          - world
          - hash-table-update
            - world
            - node
            - lambda location
              - # Wouldn't we make it easier on ourselves by having
                the children of location be a tetradic vector with
                left, right, up and down? Yes, a thousand times yes!
              - # If barabási dictates add a child, add it; otherwise,
                take a random path. This random path could backtrack,
                or it could continue on.
              - # Keep adding until we have the requisite number of
                nodes.
              - let
                - degrees-from
                  - vector-fold
                  - lambda (i degrees passage) if (no-passage? passage)
                    degrees (+ degrees 1)
                  - 0
                  - location-passages location
                - if
                  - and
                    - &lt; degrees-from 4
                    - &lt; (random-real) (/ degrees-from n-degress)
                  - let
                    - directions-from
                      - vector-fold
                        - lambda (direction directions-from passage) if
                          (no-passage? passage) (cons direction
                          directions-from) directions-from
                        - '()
                        - location-passages location
                    - direction (list-ref directions-from (random
                      (length directions-from)))
                      - let new-node (make-node)
                        - # Maybe biconnect! can take care of adding the
                          node to the world?
                        - biconnect! world node new-node direction
                        - iter new-node (- n-nodes 1) (+ n-degrees 2)
                  - let
                    - passages-from
                      - vector-fold
                        - lambda (direction passages-from passage) if
                          (passage? passage) (cons passage
                          passages-from) passages-from
                        - '()
                        - location-passages location
                    - passage (list-ref
            - make-location
              - dirty
              - #(no-passage no-passage no-passage no-passage)
</pre>



<pre class="example">(use aima
     aima-vacuum
     debug
     files
     posix
     (prefix random-bsd bsd-)
     srfi-69
     stack
     vector-lib)

(define-record no-passage)
(define no-passage (make-no-passage))
(define passage? (complement no-passage?))

(define up 2)
(define up? (cute = &lt;&gt; 2))

(define down 3)
(define down? (cute = &lt;&gt; 3))

(define-record location
  status
  neighbors)

(define-record-printer location
  (lambda (location output)
    (display (record-&gt;vector location) output)))

(define (copy-location location)
  (make-location (location-status location)
                 (vector-copy (location-neighbors location))))

(define (copy-world world)
  (let ((world (hash-table-copy world)))
    (hash-table-walk
     world
     (lambda (name location)
       (hash-table-update!
        world
        name
        copy-location)))
    world))

(define make-node gensym)

(define (random-direction) (bsd-random 4))

(define (reverse-direction direction)
  (cond ((left? direction) right)
        ((right? direction) left)
        ((up? direction) down)
        ((down? direction) up)))

(define (make-dirty-location)
  (make-location dirty
                 (vector no-passage
                         no-passage
                         no-passage
                         no-passage)))

(define (connect! world connectend connector direction)
    (hash-table-update!/default
     world
     connectend
     (lambda (location)
       (vector-set! (location-neighbors location) direction connector)
       location)
     (make-dirty-location))
    (hash-table-update!/default
     world
     connector
     (lambda (location)
       (vector-set! (location-neighbors location)
                    (reverse-direction direction)
                    connectend)
       location)
     (make-dirty-location)))

(define (make-seed-world)
  (let ((world (make-hash-table))
        (start (make-node))
        (neighbor (make-node)))
    (connect! world start neighbor (random-direction))
    world))

(define (random-start world)
  (let ((nodes (hash-table-keys world)))
    (list-ref nodes (bsd-random-integer (length nodes)))))

(define (make-randomized-reflex-graph-agent start)
  (make-reflex-agent
   start
   (lambda (location clean?)
     (if clean?
         (list-ref '(left right up down) (random-direction))
         'suck))))

(define (count-nodes world)
  (length (hash-table-keys world)))

(define (count-degrees world)
  (hash-table-fold
   world
   (lambda (node location n-degrees)
     (+ n-degrees (vector-count
                   (lambda (direction neighbor)
                     (passage? neighbor))
                   (location-neighbors location))))
   0))

(define (n-neighbors location)
  (vector-fold
   (lambda (direction n-neighbors neighbor)
     (if (no-passage? neighbor)
         n-neighbors
         (+ n-neighbors 1)))
   0
   (location-neighbors location)))

(define default-n-nodes (make-parameter 20))

;;; This, of course, won't produce any cycles.
(define make-preferential-depth-first-world
  (case-lambda
   (() (make-preferential-depth-first-world (default-n-nodes)))
   ((n-nodes)
    (let* ((world (make-seed-world))
           (start (random-start world)))
      (let iter ((node start)
                 (n-nodes
                  (max 0 (- n-nodes (count-nodes world))))
                 (n-degrees (count-degrees world)))
        (if (zero? n-nodes)
            world
            (let ((location
                   (hash-table-ref/default
                    world
                    node
                    (make-dirty-location))))
              (let ((n-neighbors (n-neighbors location)))
                (if (and (&lt; n-neighbors 4)
                         (&lt; (bsd-random-real) (/ n-neighbors n-degrees)))
                    (let* ((new-directions
                            (vector-fold
                             (lambda (direction directions neighbor)
                               (if (no-passage? neighbor)
                                   (cons direction directions)
                                   directions))
                             '()
                             (location-neighbors location)))
                           (new-direction
                            (list-ref
                             new-directions
                             (bsd-random (length new-directions)))))
                      ;; To make this Barabási-like, we could try to
                      ;; pick a preëxisting node; and, failing that,
                      ;; produce one.
                      ;;
                      ;; Why not just produce a direction-sensitive
                      ;; Barabási? Now that we have neighbors as a
                      ;; vector, it should be less unpleasant.
                      ;;
                      ;; To connect this node to a preëxisting one,
                      ;; however; we'd have to find nodes with
                      ;; compatible, available directions.
                      ;;
                      ;; We could produce a tree, of course, and
                      ;; randomly create appropriate cycles.
                      (let ((new-node (make-node)))
                        (connect! world node new-node new-direction)
                        (iter new-node (- n-nodes 1) (+ n-degrees 2))))
                    (let* ((neighbors
                            (vector-fold
                             (lambda (direction neighbors neighbor)
                               (if (passage? neighbor)
                                   (cons neighbor neighbors)
                                   neighbors))
                             '()
                             (location-neighbors location)))
                           (neighbor
                            (list-ref neighbors
                                      (bsd-random (length neighbors)))))
                      (iter neighbor n-nodes n-degrees)))))))))))

(define default-width (make-parameter 1600))

(define default-height (make-parameter 900))

(define default-font-size (make-parameter 48.0))

(define default-title (make-parameter #f))

;;; Height and width are in pixels.
(define write-dot-preamble
  (case-lambda
   ((agent step)
    (write-dot-preamble agent
                        step
                        (default-width)
                        (default-height)
                        (default-font-size)
                        (default-title)))
   ((agent step width height font-size title)
    (display "digraph G {")
    (display "node [style=filled, fontname=monospace];")
    (display "edge [fontname=monospace];")
    (if (and width height)
        (begin
          (format #t "graph [fontsize=~a, ratio=fill];" font-size)
          ;; Phew: viewports are specified in points at 72 per inch;
          ;; size is specified in pixels at 96 per inch.
          (let ((width-in-inches (/ width 96))
                (height-in-inches (/ height 96)))
            (format #t "graph [viewport=\"~a,~a\", size=\"~a,~a!\"];"
                    (* width-in-inches 72)
                    (* height-in-inches 72)
                    width-in-inches
                    height-in-inches))))
    (if step
        (format #t "graph [label=\"~aScore: ~a; step: ~a\"]"
                (if title (format "~a\\n" title) "")
                (agent-score agent)
                step)))))

(define (write-dot-nodes world agent)
  (hash-table-walk
   world
   (lambda (name location)
     (let ((color
            (cond ((eq? (agent-location agent) name) "green")
                  ((clean? (location-status location)) "white")
                  (else "gray"))))
       (format #t "~a [fillcolor=~a];" name color)))))

(define (write-dot-edges world)
  (hash-table-walk
   world
   (lambda (name location)
     (let ((left (vector-ref (location-neighbors location) left))
           (right (vector-ref (location-neighbors location) right))
           (up (vector-ref (location-neighbors location) up))
           (down (vector-ref (location-neighbors location) down)))
       (if (passage? left)
           (format #t "~a-&gt;~a [label=l];" name left))
       (if (passage? right)
           (format #t "~a-&gt;~a [label=r];" name right))
       (if (passage? up)
           (format #t "~a-&gt;~a [label=u];" name up))
       (if (passage? down)
           (format #t "~a-&gt;~a [label=d];" name down))))))

(define (write-dot-postscript) (display "}\n"))

(define write-world-as-dot
  (case-lambda
   ((world agent) (write-world-as-dot world agent #f))
   ((world agent step)
    (write-world-as-dot world
                        agent
                        step
                        (default-width)
                        (default-height)
                        (default-font-size)
                        (default-title)))
   ((world agent step width height font-size title)
    (write-dot-preamble agent step width height font-size title)
    (write-dot-nodes world agent)
    (write-dot-edges world)
    (write-dot-postscript))))

(define (write-world-as-pdf world agent pdf)
  (receive (input output id)
    (process "neato" `("-Tpdf" "-o" ,pdf))
    (with-output-to-port output
      ;; Do we really need a blank label, for some reason?
      (lambda () (write-world-as-dot world agent #f #f #f)))
    (flush-output output)
    (close-output-port output)
    (close-input-port input)))

(define (display-pdf pdf)
  (system* "evince -s ~a" pdf))

(define write-world-as-gif
  (case-lambda
   ((world agent frame gif)
    (write-world-as-gif world
                        agent
                        frame
                        gif
                        (default-width)
                        (default-height)
                        (default-font-size)
                        (default-title)))
   ((world agent frame gif width height font-size title)
    (receive (input output id)
      (process "neato" `("-Tgif" "-o" ,gif))
      (with-output-to-port output
        (lambda () (write-world-as-dot world
                                  agent
                                  frame
                                  width
                                  height
                                  font-size
                                  title)))
      (flush-output output)
      (close-output-port output)
      (close-input-port input)))))

(define (make-graph-environment world agent)
  (lambda ()
    (let* ((node (agent-location agent))
           (location (hash-table-ref world node))
           (action ((agent-program agent)
                    node
                    (clean? (location-status location)))))
      (debug-print "agent-action" action)
      (case action
        ((left)
         (let ((left (vector-ref (location-neighbors location) left)))
           (if (passage? left)
               (agent-location-set! agent left))))
        ((right)
         (let ((right (vector-ref (location-neighbors location) right)))
           (if (passage? right)
               (agent-location-set! agent right))))
        ((up)
         (let ((up (vector-ref (location-neighbors location) up)))
           (if (passage? up)
               (agent-location-set! agent up))))
        ((down)
         (let ((down (vector-ref (location-neighbors location) down)))
           (if (passage? down)
               (agent-location-set! agent down))))
        ((noop))
        ((suck)
         (location-status-set! (hash-table-ref world node) clean))
        (else (error "graph-environment -- Unknown action"))))))

(define (make-graph-performance-measure world agent)
  (lambda ()
    (let ((clean-locations
           ;; Quicker with map and apply?
           (hash-table-fold
            world
            (lambda (name location clean-locations)
              (if (clean? (location-status location))
                  (+ clean-locations 1)
                  clean-locations))
            0)))
      (agent-score-set! agent (+ (agent-score agent) clean-locations)))))

(define make-graph-animating-environment
  (case-lambda
   ((world agent directory)
    (make-graph-animating-environment world
                                      agent
                                      directory
                                      (default-width)
                                      (default-height)
                                      (default-font-size)
                                      (default-title)))
   ((world agent directory width height font-size title)
    (let ((frame 0))
      (lambda ()
        (let ((gif (make-pathname directory (number-&gt;string frame) "gif")))
          (write-world-as-gif world
                              agent
                              frame
                              gif
                              width
                              height
                              font-size
                              title))
        (set! frame (+ frame 1)))))))

;;; I think make-step-limited-environment is a special case of
;;; make-finalizing-environment with noop.
;;;
;;; This probably belongs in aima; should we preserve the
;;; step-limited-environment as a specialization of this?
(define (make-finalizing-environment finalizer final-step)
  (let ((step 0))
    (lambda ()
      (set! step (+ step 1))
      (let ((continue? (&lt; step final-step)))
        (if (not continue?) (finalizer))
        continue?))))

(define (make-animation-finalizer directory file)
  (lambda ()
    (system* "rm -fv ~a.gif" file)
    (system* "convert $(find ~a -type f | sort -k 1.~a -n) -loop 0 ~a.gif"
             directory
             (+ (string-length directory) 2)
             file)
    (system* "identify ~a/0.gif" directory)
    (system* "mencoder ~a.gif -ovc lavc -o ~a.avi" file file)))

(define (make-composite-animation-finalizer combinandum combinator file)
  (let ((composite-directory (create-temporary-directory)))
    (system* "cd ~a &amp;&amp; for i in *; do echo $i; convert +append $i ~a/$i ~a/$i; done"
             combinandum
             combinator
             composite-directory)
    (make-animation-finalizer composite-directory file)))

(define (make-unknown-location clean?)
  (make-location (if clean? clean dirty)
                 (vector unknown unknown unknown unknown)))

(define (undiscovered-directions location)
  (vector-fold
   (lambda (direction undiscovered-directions neighbor)
     (if (unknown? neighbor)
         (cons direction undiscovered-directions)
         undiscovered-directions))
   '()
   (location-neighbors location)))

(define (reverse-move move)
  (case move
    ((left) 'right)
    ((right) 'left)
    ((up) 'down)
    ((down) 'up)))

(define (direction-&gt;move direction)
  (list-ref '(left right up down) direction))

(define (move-&gt;direction move)
  (case move
    ((left) left)
    ((right) right)
    ((up) up)
    ((down) down)))

(define-record cycle)
(define cycle (make-cycle))

;;; Dealing with all this move-location punning makes things complex;
;;; we can clean this up a little bit by writing some germane
;;; abstractions on the world.
;;;
;;; We're not dealing with cycles yet, by the way; does this entail
;;; determining whether or not a new node is accounted for in the
;;; world? I believe so.
(define (make-stateful-graph-agent start)
  (make-reflex-agent
   start
   (let ((world (make-hash-table))
         (nodes (list-&gt;stack (list start)))
         (moves (make-stack)))
     (lambda (node clean?)
       (if (stack-empty? nodes)
           'noop
           (if (not clean?)
               'suck
               (let ((location
                      (hash-table-ref/default
                       world
                       node
                       (make-unknown-location clean?))))
                 ;; The following is general house-keeping on the state.
                 (if (stack-empty? moves)
                     ;; We're dealing with an uninitialized agent: set
                     ;; the world. This could also be a terminal
                     ;; agent, couldn't it? Is there a better place to
                     ;; initialize?
                     (hash-table-set! world node location)
                     ;; We need to distinguish the case, apparently,
                     ;; where we've just backtracked; this isn't quite
                     ;; the same as a fail-to-move.
                     ;;
                     ;; In 2.12, when we're dealing with a bump
                     ;; sensor, when don't have to play these games
                     ;; with an implicit bump.
                     (let ((last-move (stack-peek moves)))
                       (if (eq? last-move 'backtrack)
                           ;; Our position is the result of
                           ;; backtracking; remove the special
                           ;; backtracking move.
                           (stack-pop! moves)
                           (if (eq? (stack-peek nodes) node)
                               ;; We tried to move but could not; mark the
                               ;; last direction as no-passage.
                               (let ((last-move (stack-pop! moves)))
                                 (vector-set! (location-neighbors location)
                                              (move-&gt;direction last-move)
                                              no-passage))
                               (let* ((last-node (stack-peek nodes))
                                      ;; Need to replace hash-table-ref, &amp;c.
                                      ;; with something more germane.
                                      (last-location
                                       (hash-table-ref world last-node)))
                                 (if (hash-table-exists? world node)
                                     ;; Cycle detected! Push the
                                     ;; cycle-sentinel.
                                     (stack-push! nodes cycle)
                                     (begin
                                       ;; This is a new node: add it
                                       ;; to the world.
                                       (hash-table-set! world node location)
                                       ;; Also, add it to the list of
                                       ;; interesting nodes.
                                       (stack-push! nodes node)))
                                 ;; This location's reverse-move points to
                                 ;; the last node.
                                 (vector-set! (location-neighbors location)
                                              (move-&gt;direction
                                               (reverse-move last-move))
                                              last-node)
                                 ;; The last location's move points to
                                 ;; this node.
                                 (vector-set! (location-neighbors
                                               last-location)
                                              (move-&gt;direction last-move)
                                              node))))))
                 ;; Are there any other undiscovered passages?
                 (let ((new-moves (map direction-&gt;move
                                       (undiscovered-directions location))))
                   (if (or (cycle? (stack-peek nodes))
                           (null? new-moves))
                       (begin
                         ;; Remove this node from the interesting
                         ;; nodes: it's been thoroughly explored.
                         (stack-pop! nodes)
                         (if (stack-empty? moves)
                             ;; No moves lest; let's rest. This may change
                             'noop
                             (let ((move (stack-pop! moves)))
                               ;; Push the special backtrack move onto the
                               ;; stack; this helps us distinguish the
                               ;; backtracking case from the case where
                               ;; we've hit a wall.
                               ;;
                               ;; The bump-sensor should obviate the
                               ;; need for this, I think; or not.
                               (stack-push! moves 'backtrack)
                               ;; Go back the way we came.
                               (reverse-move move)))) 
                       (let ((move (list-ref new-moves
                                             (bsd-random (length new-moves)))))
                         (stack-push! moves move)
                         move))))))))))

(define (make-known-world)
  (let ((world (make-hash-table))
        (a (make-node))
        (b (make-node))
        (c (make-node))
        (d (make-node))
        (e (make-node))
        (f (make-node)))
    (connect! world 'a 'b right)
    (connect! world 'b 'c down)
    (connect! world 'b 'd right)
    (connect! world 'd 'e down)
    (connect! world 'e 'f down)
    (connect! world 'f 'e right)
    (connect! world 'f 'a down)
    world))

(define default-file (make-parameter "graph"))

(define simulate-graph
  (case-lambda
   ((world agent)
    (simulate-graph world agent (default-steps)))
   ((world agent steps)
    (simulate-graph world
                    agent
                    steps
                    (default-width)
                    (default-height)
                    (default-font-size)
                    (default-title)
                    (default-file)))
   ((world agent steps width height font-size title file)
    (let ((directory (create-temporary-directory)))
      (simulate
       ;; Order of composition matters, apparently; be thoughtful.
       (compose-environments
        (make-step-limited-environment steps)
        (make-debug-environment agent)
        (make-graph-environment world agent)
        (make-graph-performance-measure world agent)))
      directory))))

(define simulate-graph/animation
  (case-lambda
   ((world agent)
    (simulate-graph/animation world agent (default-steps)))
   ((world agent steps)
    (simulate-graph/animation world
                    agent
                    steps
                    (default-width)
                    (default-height)
                    (default-font-size)
                    (default-title)
                    (default-file)))
   ((world agent steps width height font-size title file)
    (let ((directory (create-temporary-directory)))
      (simulate
       ;; Order of composition matters, apparently; be thoughtful.
       (compose-environments
        (make-step-limited-environment steps)
        ;; Can't this contain its finalizer? Maybe even give it the
        ;; terminal frame?
        (make-graph-animating-environment world
                                          agent
                                          directory
                                          width
                                          height 
                                          font-size
                                          title)
        (make-finalizing-environment
         (make-animation-finalizer directory file)
         steps)
        (make-debug-environment agent)
        (make-graph-environment world agent)
        (make-graph-performance-measure world agent)))
      directory))))

(define (simulate-comparatively world agent steps width height font-size title)
  (let ((directory (create-temporary-directory)))
    (simulate
     ;; Order of composition matters, apparently; be thoughtful.
     (compose-environments
      (make-step-limited-environment steps)
      ;; Can't this contain its finalizer? Maybe even give it the
      ;; terminal frame?
      (make-graph-animating-environment world
                                        agent
                                        directory
                                        width
                                        height 
                                        font-size
                                        title)
      (make-debug-environment agent)
      (make-graph-environment world agent)
      (make-graph-performance-measure world agent)))
    directory))

;;; We should generalize this.
(define compare-graphs
  (case-lambda
   ((world agent-one title-one agent-two title-two composite-file)
    (compare-graphs world
                    agent-one
                    title-one
                    agent-two
                    title-two
                    composite-file
                    (default-steps)
                    (/ (default-width) 2)
                    (default-height)
                    (/ (default-font-size) 2)))
   ((world
     agent-one
     title-one
     agent-two
     title-two
     composite-file
     steps
     width
     height
     font-size)
    (let ((directory-one
           (simulate-comparatively (copy-world world)
                                   agent-one
                                   steps
                                   width
                                   height
                                   font-size
                                   title-one))
          (directory-two
           (simulate-comparatively world
                                   agent-two
                                   steps
                                   width
                                   height
                                   font-size
                                   title-two)))
      (let ((composite-directory (create-temporary-directory)))
        (system* "cd ~a &amp;&amp; for i in *; do echo $i; convert +append $i ~a/$i ~a/$i; done"
                 directory-one
                 directory-two
                 composite-directory)
        ((make-animation-finalizer composite-directory composite-file)))))))

(let* ((world (make-preferential-depth-first-world 20))
       (start (random-start world)))
  (let ((stateful-agent (make-stateful-graph-agent start))
        (random-agent (make-randomized-reflex-graph-agent start)))
    (parameterize ((default-steps 10)
                   (randomize! bsd-randomize)
                   (random-seed 0))
      (compare-graphs world
                      stateful-agent
                      "Stateful agent"
                      random-agent
                      "Random agent"
                      "composite-agent-harro"))))

</pre>

</div>

</div>

<div id="outline-container-35-5-3" class="outline-4">
<h4 id="sec-35-5-3"><span class="section-number-4">35.5.3</span> <span class="done DONE">DONE</span> Allow specification of random-seeds</h4>
<div class="outline-text-4" id="text-35-5-3">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:44</span></span><br/>
</p></div>

</div>

<div id="outline-container-35-5-4" class="outline-4">
<h4 id="sec-35-5-4"><span class="section-number-4">35.5.4</span> <span class="done DONE">DONE</span> Stateful graph agent</h4>
<div class="outline-text-4" id="text-35-5-4">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:44</span></span><br/>
    The stateful graph agent has to divine indirectly that it
    encountered an obstacle based on whether or not the last action
    was movement and whether or not it's in the same position (this
    will change slightly with the bump sensor in <a href="#sec-27">2.12</a>).
</p>



<pre class="example">- make-stateful-graph-agent start
  - let
    - world make-hash-table
      - # This is going to have to be a stack: we're iterating here,
        after all. Ah, the problem is that we can't recurse back up the
        tree because the relative directions are asymmetrical.
      - # I knew this would bite me in the ass; or is there a more
        expensive way to backtrack? Can't e.g. try random directions
        because there's no way back.
      - # Is it possible, somehow, to progress through the world via
        random walk; turning away when we encounter a taken path? No, we
        need to be able to backtrack.
      - # Fuck it: just wrote a preferential depth-first world-generator
        that's not quite as good as Barabási-Albert in the sense that it
        doesn't generate cycles.
      - # On the other hand, wouldn't it have been equivalent to store
        the desirable node and the direction required to get there? I
        still need the inverse of the current direction to backtrack, I
        believe.
      - # It would appear as though we're going to have to push a list
        of interesting vertices onto the stack as well as the actions
        required to get here; this notion, therefore, of visit node
        entails: back-tracking until we find it.
    - path ()
    - visitanda (start)
    - last-action #f
    - lambda node clean?
      - if null? visitanda
        - noop
        - if dirty?
          - # This is where pre-/post-order becomes interesting;
            pre-order it, so we can maximize our score.
          - clean
          - begin
            - hash-table-update!/default
              - world
              - node
              - lambda location
                - location-set-status (if clean? clean dirty)
                - let*
                  - neighbors (location-neighbors location)
                    - if eq? last-node node
                      - case last-action
                        - left vector-set! neighbors left no-passage
                        - right vector-set! neighbors right no-passage
                        - up vector-set! neighbors up no-passage
                        - down vector-set! neighbors down no-passage
                      - let last-node (car visitandum)
                        - case last-action
                          - left vector-set! neighbors right last-node
                          - right vector-set! neighbors left last-node
                          - up vector-set! neighbors down last-node
                          - down vector-set! neighbors up last-node
                        - set! visitanda (cons node visitanda)
                - location
              - # Let's nominate this make-unknown-location (as opposed to:
                make-dirty-location).
              - make-location clean? #(unknown unknown unknown unknown)

</pre>

</div>

</div>

<div id="outline-container-35-5-5" class="outline-4">
<h4 id="sec-35-5-5"><span class="section-number-4">35.5.5</span> <span class="done CANCELED">CANCELED</span> <code>make-equilibrium-limited-environment</code></h4>
<div class="outline-text-4" id="text-35-5-5">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:44</span></span><br/>
    Compare the delta on scores for some sort of equilibrium; or lack
    of movement?
</p></div>

</div>

<div id="outline-container-35-5-6" class="outline-4">
<h4 id="sec-35-5-6"><span class="section-number-4">35.5.6</span> <span class="done DONE">DONE</span> Compare stateful and randomized reflex agents.</h4>
<div class="outline-text-4" id="text-35-5-6">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-16 Mon 12:43</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-07-16 Mon 12:43</span></span> <br/>
      See <a href="http://youtu.be/B28ay_zSnoY">http://youtu.be/B28ay<sub>zSnoY</sub></a>.
</li>
</ul>

</div>

</div>

<div id="outline-container-35-5-7" class="outline-4">
<h4 id="sec-35-5-7"><span class="section-number-4">35.5.7</span> <span class="done DONE">DONE</span> Figure out the correct aspect ratio for youtube.</h4>
<div class="outline-text-4" id="text-35-5-7">

<p>     <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-07-08 Sun 05:42</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-07-08 Sun 05:42</span></span> <br/>
      Involves the viewport, &amp;c.
</li>
</ul>

<p>    See <a href="http://support.google.com/youtube/bin/static.pyhl=en&amp;guide=1728585&amp;topic=1728569&amp;page=guide.cs">this</a>; ideally, this would start at graphviz.
</p></div>
</div>

</div>

<div id="outline-container-35-6" class="outline-3">
<h3 id="sec-35-6"><span class="section-number-3">35.6</span> <span class="todo TODO">TODO</span> Should we structure this somehow as a blog instead of an org-doc?</h3>
<div class="outline-text-3" id="text-35-6">

<p>   Can we create a jekyll-based blog from this org-doc and push it to
   the github pages?
</p>
<p>
   <i>That</i> would be beautiful: we could get rid of the generated pdf
   and might not be confined to the relative obscurity of org
   (relative to, say, html).
</p></div>

</div>

<div id="outline-container-35-7" class="outline-3">
<h3 id="sec-35-7"><span class="section-number-3">35.7</span> <span class="todo TODO">TODO</span> Some sort of blog post or other publicity?</h3>
<div class="outline-text-3" id="text-35-7">

</div>

</div>

<div id="outline-container-35-8" class="outline-3">
<h3 id="sec-35-8"><span class="section-number-3">35.8</span> <span class="todo TODO">TODO</span> Find a reasonable pseudocode package in \LaTeX.</h3>
<div class="outline-text-3" id="text-35-8">

<p>   See <a href="http://www.tex.ac.uk/cgi-bin/texfaq2htmllabel=algorithms">this survey</a>. Algorithm2e isn't bad; doesn't seem to have a
   function, though. <a href="http://www.math.washington.edu/tex-archive/help/Catalogue/entries/pseudocode.html">Pseudocode</a> seems to be relatively natural; even
   if the output is a little ugly.
</p>
<p>
   The alternative, I suppose, is straight up lists.
</p>
<p>
   Algorithm2e has to be wrapped in dollars, which sucks; also:
   bizarre camel-case macros. Looks good, otherwise. Has no functions,
   apparently, either.
</p></div>

</div>

<div id="outline-container-35-9" class="outline-3">
<h3 id="sec-35-9"><span class="section-number-3">35.9</span> <span class="todo TODO">TODO</span> Should we tangle to a bunch of text files?</h3>
<div class="outline-text-3" id="text-35-9">

<p>   Looking for an alternative to the big-ass pdf.
</p></div>

</div>

<div id="outline-container-35-10" class="outline-3">
<h3 id="sec-35-10"><span class="section-number-3">35.10</span> <span class="done DONE">DONE</span> Reimplement the Lisp environment in Scheme.</h3>
<div class="outline-text-3" id="text-35-10">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-01 Fri 03:09</span></span><br/>
  Should we try to map CLOS to <a href="http://wiki.call-cc.org/eggref/4/coops">coops</a>? Or maybe <a href="http://wiki.call-cc.org/eggref/4/tinyclos">TinyCLOS</a> would suffice.
  This takes balls. See <a href="https://github.com/klutometis/aima-chicken">aima-chicken</a>.
</p></div>

</div>

<div id="outline-container-35-11" class="outline-3">
<h3 id="sec-35-11"><span class="section-number-3">35.11</span> <span class="done DONE">DONE</span> Personal notes as footnotes.</h3>
<div class="outline-text-3" id="text-35-11">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-01 Fri 03:09</span></span><br/>
</p></div>

</div>

<div id="outline-container-35-12" class="outline-3">
<h3 id="sec-35-12"><span class="section-number-3">35.12</span> <span class="done CANCELED">CANCELED</span> Should we try to release an e.g. Wumpus World egg?</h3>
<div class="outline-text-3" id="text-35-12">

<p>   <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-06-01 Fri 03:08</span></span><br/>
</p><ul>
<li>CLOSING NOTE <span class="timestamp-wrapper"> <span class="timestamp">2012-06-01 Fri 03:08</span></span> <br/>
    This is superseded by the Chicken port of the Lisp implementation
    (aima-chicken).
</li>
</ul>

<p>  I wonder if it would be worthwhile to study the canonical Lisp
  examples.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">
<p class="footnote"><sup><a class="footnum" name="fn.1" href="#fnr.1">1</a></sup> See this <a href="http://linux.about.com/b/2007/05/16/google-machine-translation-system-beats-competition.htm">article from 2007</a> on Google's machine translation
  system: "Using a brute-force statistical approach, the Google
  machine translation team has developed top performing translation
  software to and from languages that not even one of the teams
  members understands, such as Arabic and Chinese."
</p>


<p class="footnote"><sup><a class="footnum" name="fn.2" href="#fnr.2">2</a></sup> "Let \(T\) be the lifetime of the agent (the total number of
  percepts it will receive)" (p. 47). Percepts are the granularity of
  time.
</p>


<p class="footnote"><sup><a class="footnum" name="fn.3" href="#fnr.3">3</a></sup> The current state of the environment is the same as the current
  percept, incidentally, if the environment is fully observable.
</p>


<p class="footnote"><sup><a class="footnum" name="fn.4" href="#fnr.4">4</a></sup> The agent's minds are unobservable: we have to operate in
  belief space.
</p>


<p class="footnote"><sup><a class="footnum" name="fn.5" href="#fnr.5">5</a></sup> From page 67: "The state space . . . is the set of all states
  reachable from the initial state by any sequence of actions."
</p>


<p class="footnote"><sup><a class="footnum" name="fn.6" href="#fnr.6">6</a></sup> Noöp, where one friend stays put; is apparently not an option.
  If it were, there wouldn't be cases where vacillation is required to
  convene.
</p>


<p class="footnote"><sup><a class="footnum" name="fn.7" href="#fnr.7">7</a></sup> Bizarre to me that a programmer is responsible for the <i>a   priori</i>; playing god, anyone?
</p>


<p class="footnote"><sup><a class="footnum" name="fn.8" href="#fnr.8">8</a></sup> Which of these following papers do you think he's talking
  about? Probably the latter: it carries an <i>et al.</i>
</p>


</div>
</div>
</div>

</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2012-08-20 04:49:35 PDT</p>
<p class="author">Author: </p>
<p class="creator">Org version 7.8.11 with Emacs version 24</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
